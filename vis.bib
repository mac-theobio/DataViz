@Article{augustin_quantile_2012,
  Title                    = {On quantile quantile plots for generalized linear models},
  Author                   = {Augustin, Nicole H. and Sauleau, Erik-André and Wood, Simon N.},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2012},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {2404--2409},
  Volume                   = {56},

  Doi                      = {10.1016/j.csda.2012.01.026},
  ISSN                     = {01679473},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S0167947312000692},
  Urldate                  = {2013-07-01}
}

@InProceedings{bateman_useful_2010,
  Title                    = {Useful junk?: the effects of visual embellishment on comprehension and memorability of charts},
  Author                   = {Bateman, Scott and Mandryk, Regan L. and Gutwin, Carl and Genest, Aaron and McDine, David and Brooks, Christopher},
  Booktitle                = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
  Year                     = {2010},
  Pages                    = {2573--2582},
  Publisher                = {ACM},

  Shorttitle               = {Useful junk?}
}

@Misc{beus_redesign_2018,
  Title                    = {Redesign of a truly bananas chart},

  Author                   = {de Beus, Thomas},
  Month                    = jan,
  Year                     = {2018},

  Abstract                 = {On Twitter, I stumbled upon this horrendous 3D bar chart. When looking at the data, it might have been made in 2005. Data visualisation as…},
  Journal                  = {Colourful Facts},
  Url                      = {https://medium.com/tdebeus/redesign-of-a-truly-bananas-chart-1617f930808d},
  Urldate                  = {2018-01-09}
}

@Misc{debeus_contribute_2017,
  Title                    = {colourful-facts repository},

  Author                   = {de Beus, Thomas},
  Month                    = jul,
  Note                     = {original-date: 2017-07-24T11:35:18Z},
  Year                     = {2017},

  Url                      = {https://github.com/thomasdebeus/colourful-facts},
  Urldate                  = {2018-01-09}
}

@Article{borkin_beyond_2016,
  Title                    = {Beyond {Memorability}: {Visualization} {Recognition} and {Recall}},
  Author                   = {Borkin, Michelle A. and Bylinskii, Zoya and Kim, Nam Wook and Bainbridge, Constance May and Yeh, Chelsea S. and Borkin, Daniel and Pfister, Hanspeter and Oliva, Aude},
  Journal                  = {IEEE Transactions on Visualization and Computer Graphics},
  Year                     = {2016},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {519--528},
  Volume                   = {22},

  Doi                      = {10.1109/TVCG.2015.2467732},
  ISSN                     = {1077-2626},
  Shorttitle               = {Beyond {Memorability}},
  Url                      = {http://ieeexplore.ieee.org/document/7192646/},
  Urldate                  = {2018-02-09}
}

@Misc{broman_creating_2017,
  Title                    = {Creating effective figures and tables},

  Author                   = {Broman, Karl W},
  Year                     = {2017},

  Url                      = {https://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf},
  Urldate                  = {2017-10-26}
}

@TechReport{broman_data_2017,
  Title                    = {Data organization in spreadsheets},
  Author                   = {Broman, Karl W and Woo, Kara H.},
  Institution              = {PeerJ Preprints},
  Year                     = {2017},
  Month                    = aug,
  Note                     = {DOI: 10.7287/peerj.preprints.3183v1},

  Abstract                 = {Spreadsheets are widely used software tools for data entry, storage, analysis, and visualization. Focusing on the data entry and storage aspects, this paper offers practical recommendations for organizing spreadsheet data to reduce errors and ease later analyses. The basic principles are: be consistent, write dates like YYYY-MM-DD, don't leave any cells empty, put just one thing in a cell, organize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row), create a data dictionary, don't include calculations in the raw data files, don't use font color or highlighting as data, choose good names for things, make backups, use data validation to avoid data entry errors, and save the data in plain text file.},
  Language                 = {eng},
  Url                      = {https://peerj.com/preprints/3183v1/},
  Urldate                  = {2017-08-31}
}

@Article{buja_statistical_2009,
  Title                    = {Statistical inference for exploratory data analysis and model diagnostics},
  Author                   = {Buja, A. and Cook, D. and Hofmann, H. and Lawrence, M. and Lee, E.-K. and Swayne, D. F. and Wickham, H.},
  Journal                  = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  Year                     = {2009},

  Month                    = oct,
  Number                   = {1906},
  Pages                    = {4361--4383},
  Volume                   = {367},

  Doi                      = {10.1098/rsta.2009.0120},
  ISSN                     = {1364-503X},
  Url                      = {http://rsta.royalsocietypublishing.org/cgi/doi/10.1098/rsta.2009.0120}
}

@Misc{camoes_stephen_2010,
  Title                    = {Stephen {Few}, {Data} {Visualization}, {Eye} {Candy} and {Pie} {Charts}},

  Author                   = {Camoes, Jorge},

  Url                      = {http://www.excelcharts.com/blog/stephen-few-data-visualization-eye-candy-and-the-pie/},
  Urldate                  = {2010-09-30}
}

@Misc{camoes_what-would-tufte-say_nodate,
  Title                    = {The "what-would-{Tufte}-say" syndrome},

  Author                   = {Camoes, Jorge},

  Abstract                 = {An alarming level of the "what-would-Tufte-say" syndrome can be found in this post and some of its comments discussing a New York Times's infographic. This syndrome has some recognizable features like the extensive use of "chart junk", "lie factor" or other terms and expressions coined by Tufte that reveal a somewhat},
  Journal                  = {The Excel Charts Blog},
  Url                      = {http://www.excelcharts.com/blog/the-what-would-tufte-say-syndrome/},
  Urldate                  = {2017-01-10}
}

@Book{cleveland_visualizing_1993,
  Title                    = {Visualizing {Data}},
  Author                   = {Cleveland, William},
  Publisher                = {Hobart Press},
  Year                     = {1993},

  Address                  = {Summit, NJ}
}

@Article{cleveland_graphical_1987,
  Title                    = {Graphical {Perception}: {The} {Visual} {Decoding} of {Quantitative} {Information} on {Graphical} {Displays} of {Data}},
  Author                   = {Cleveland, William S. and McGill, Robert},
  Journal                  = {Journal of the Royal Statistical Society. Series A (General)},
  Year                     = {1987},

  Month                    = jan,
  Note                     = {ArticleType: research-article / Full publication date: 1987 / Copyright © 1987 Royal Statistical Society},
  Number                   = {3},
  Pages                    = {192--229},
  Volume                   = {150},

  Abstract                 = {Studies in graphical perception, both theoretical and experimental, provide a scientific foundation for the construction area of statistical graphics. From these studies a paradigm that has important applications for practice has begun to emerge. The paradigm is based on elementary codes: Basic geometric and textural aspects of a graph that encode the quantitative information. The methodology that can be invoked to study graphical perception is illustrated by an investigation of the shape parameter of a two-variable graph, a topic that has had much discussion, but little scientific study, for at least 70 years.},
  Doi                      = {10.2307/2981473},
  ISSN                     = {00359238},
  Shorttitle               = {Graphical {Perception}},
  Url                      = {http://www.jstor.org/stable/2981473},
  Urldate                  = {2011-01-03}
}

@Article{cleveland_graphical_1984,
  Title                    = {Graphical {Perception}: {Theory}, {Experimentation}, and {Application} to the {Development} of {Graphical} {Methods}},
  Author                   = {Cleveland, William S. and McGill, Robert},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1984},
  Note                     = {ArticleType: research-article / Full publication date: Sep., 1984 / Copyright © 1984 American Statistical Association},
  Number                   = {387},
  Pages                    = {531--554},
  Volume                   = {79},

  Abstract                 = {The subject of graphical methods for data analysis and for data presentation needs a scientific foundation. In this article we take a few steps in the direction of establishing such a foundation. Our approach is based on graphical perception-the visual decoding of information encoded on graphs-and it includes both theory and experimentation to test the theory. The theory deals with a small but important piece of the whole process of graphical perception. The first part is an identification of a set of elementary perceptual tasks that are carried out when people extract quantitative information from graphs. The second part is an ordering of the tasks on the basis of how accurately people perform them. Elements of the theory are tested by experimentation in which subjects record their judgments of the quantitative information on graphs. The experiments validate these elements but also suggest that the set of elementary tasks should be expanded. The theory provides a guideline for graph construction: Graphs should employ elementary tasks as high in the ordering as possible. This principle is applied to a variety of graphs, including bar charts, divided bar charts, pie charts, and statistical maps with shading. The conclusion is that radical surgery on these popular graphs is needed, and as replacements we offer alternative graphical forms-dot charts, dot charts with grouping, and framed-rectangle charts.},
  Doi                      = {10.2307/2288400},
  ISSN                     = {01621459},
  Shorttitle               = {Graphical {Perception}},
  Url                      = {http://www.jstor.org/stable/2288400},
  Urldate                  = {2011-01-03}
}

@Article{dawson_how_2011,
  Title                    = {How significant is a boxplot outlier},
  Author                   = {Dawson, Robert},
  Journal                  = {Journal of Statistics Education},
  Year                     = {2011},
  Number                   = {2},
  Pages                    = {1--12},
  Volume                   = {19}
}

@Book{doumont_trees_2009,
  Title                    = {Trees, {Maps}, and {Theorems} {Effective} {Communication} for {Rational} {Minds}},
  Author                   = {Doumont, Jean-Luc},
  Publisher                = {Principiae},
  Year                     = {2009},

  Address                  = {Kraainem},

  Abstract                 = {The book comprises five parts: first, fundamentals, then the written documents, oral presentations, and graphical displays, and finally the application of these ideas to more specific types of document.},
  ISBN                     = {978-90-813677-0-7},
  Language                 = {English}
}

@Misc{drang_drum_2017,
  Title                    = {Drum and {Tufte} and grids and axes - {All} this},

  Author                   = {{Dr. Drang}},

  Url                      = {http://leancrew.com/all-this/2017/12/drum-tufte-grids-axes/},
  Urldate                  = {2018-01-01}
}

@Misc{drum_yet_2017,
  Title                    = {Yet more chart geekery},

  Author                   = {Drum, Kevin},

  Journal                  = {Mother Jones},
  Url                      = {http://www.motherjones.com/kevin-drum/2017/12/yet-more-chart-geekery/},
  Urldate                  = {2018-01-01}
}

@Misc{elliott_39_2016,
  Title                    = {39 studies about human perception in 30 minutes},

  Author                   = {Elliott, Kennedy},
  Month                    = may,
  Year                     = {2016},

  Abstract                 = {These are my speaker notes from a talk I gave at OpenVis in April 2016. Originally this talk was supposed to be called “Everything we know…},
  Journal                  = {Medium},
  Url                      = {https://medium.com/@kennelliott/39-studies-about-human-perception-in-30-minutes-4728f9e31a73},
  Urldate                  = {2018-01-16}
}

@Book{few_now_2009,
  Title                    = {Now {You} {See} {It}: {Simple} {Visualization} {Techniques} for {Quantitative} {Analysis}},
  Author                   = {Few, Stephen},
  Publisher                = {Analytics Press},
  Year                     = {2009},
  Edition                  = {1st},
  Month                    = apr,

  ISBN                     = {0-9706019-8-0},
  Shorttitle               = {Now {You} {See} {It}}
}

@Article{friendly_effect_2003,
  Title                    = {Effect ordering for data displays},
  Author                   = {Friendly, Michael and Kwan, Ernest},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2003},

  Month                    = aug,
  Number                   = {4},
  Pages                    = {509--539},
  Volume                   = {43},

  Abstract                 = {This paper outlines a general framework for ordering information in visual displays (tables and graphs) according to the effects or trends which we desire to see. This idea, termed effect-ordered data displays, applies principally to the arrangement of unordered factors for quantitative data and frequency data, and to the arrangement of variables and observations in multivariate displays (star plots, parallel coordinate plots, and so forth).

As examples of this principle, we present several techniques for ordering items, levels or variables “optimally”, based on some desired criterion. All of these may be based on eigenvalue or singular-value decompositions.

Along the way, we tell some stories about data display, illustrated by graphs—some surprisingly bad, and some surprisingly good—for showing patterns, trends, and anomalies in data. We hope to raise more questions than we can provide answers for.},
  Doi                      = {10.1016/S0167-9473(02)00290-6},
  File                     = {ScienceDirect Snapshot:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/C79AU4QH/S0167947302002906.html:text/html},
  ISSN                     = {0167-9473},
  Series                   = {Data {Visualization}},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167947302002906},
  Urldate                  = {2016-01-23}
}

@Misc{gelman_infovis_2011,
  Title                    = {Infovis, infographics, and data visualization: {Where} {I}'m coming from, and where {I}'d like to go},

  Author                   = {Gelman, Andrew},
  Month                    = aug,
  Year                     = {2011},

  Abstract                 = {I continue to struggle to convey my thoughts on statistical graphics so I’ll try another approach, this time giving my own story. For newcomers to this discussion: the background is that Antony Unwin and I wrote an article on the different goals embodied in information visualization and statistical graphics, but I have difficulty communicating on …},
  Journal                  = {Statistical Modeling, Causal Inference, and Social Science},
  Language                 = {en-US},
  Shorttitle               = {Infovis, infographics, and data visualization},
  Url                      = {http://andrewgelman.com/2011/08/29/infovis-infographics-and-data-visualization-where-im-coming-from-and-where-id-like-to-go/},
  Urldate                  = {2018-02-12}
}

@Article{gelman_why_2011,
  Title                    = {Why {Tables} {Are} {Really} {Much} {Better} {Than} {Graphs}},
  Author                   = {Gelman, Andrew},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2011},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {3--7},
  Volume                   = {20},

  Doi                      = {10.1198/jcgs.2011.09166},
  ISSN                     = {1061-8600},
  Url                      = {http://pubs.amstat.org/doi/abs/10.1198/jcgs.2011.09166},
  Urldate                  = {2011-08-15}
}

@Article{gelman_scaling_2008,
  Title                    = {Scaling regression inputs by dividing by two standard deviations},
  Author                   = {Gelman, Andrew},
  Journal                  = {Statistics in Medicine},
  Year                     = {2008},

  Month                    = jul,
  Number                   = {15},
  Pages                    = {2865--2873},
  Volume                   = {27},

  Doi                      = {10.1002/sim.3107},
  ISSN                     = {02776715, 10970258},
  Language                 = {en},
  Url                      = {http://doi.wiley.com/10.1002/sim.3107},
  Urldate                  = {2018-02-04}
}

@Article{gelman_exploratory_2004,
  Title                    = {Exploratory {Data} {Analysis} for {Complex} {Models}},
  Author                   = {Gelman, Andrew},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2004},

  Month                    = dec,
  Number                   = {4},
  Pages                    = {755--779},
  Volume                   = {13},

  Doi                      = {10.1198/106186004X11435},
  ISSN                     = {1061-8600, 1537-2715},
  Language                 = {en},
  Url                      = {http://www.tandfonline.com/doi/abs/10.1198/106186004X11435},
  Urldate                  = {2018-01-24}
}

@Article{gelman_lets_2002,
  Title                    = {Let's practice what we preach: turning tables into graphs},
  Author                   = {Gelman, Andrew and Pasarica, Cristian and Dodhia, Rahul},
  Journal                  = {The American Statistician},
  Year                     = {2002},
  Number                   = {2},
  Pages                    = {121--130},
  Volume                   = {56},

  Shorttitle               = {Let's practice what we preach},
  Url                      = {http://www.tandfonline.com/doi/abs/10.1198/000313002317572790},
  Urldate                  = {2016-02-01}
}

@Article{gelman_all_1999,
  Title                    = {All maps of parameter estimates are misleading},
  Author                   = {Gelman, Andrew and Price, Phillip N. and {others}},
  Journal                  = {Statistics in medicine},
  Year                     = {1999},
  Number                   = {23},
  Pages                    = {3221--3234},
  Volume                   = {18},

  Url                      = {http://www.stat.columbia.edu/~gelman/research/published/allmaps.pdf},
  Urldate                  = {2017-08-09}
}

@Article{gelman_infovis_2013,
  Title                    = {Infovis and {Statistical} {Graphics}: {Different} {Goals}, {Different} {Looks}},
  Author                   = {Gelman, Andrew and Unwin, Antony},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {2--28},
  Volume                   = {22},

  Abstract                 = {The importance of graphical displays in statistical practice has been recognized sporadically in the statistical literature over the past century, with wider awareness following Tukey's Exploratory Data Analysis and Tufte's books in the succeeding decades. But statistical graphics still occupy an awkward in-between position: within statistics, exploratory and graphical methods represent a minor subfield and are not well integrated with larger themes of modeling and inference. Outside of statistics, infographics (also called information visualization or Infovis) are huge, but their purveyors and enthusiasts appear largely to be uninterested in statistical principles. We present here a set of goals for graphical displays discussed primarily from the statistical point of view and discuss some inherent contradictions in these goals that may be impeding communication between the fields of statistics and Infovis. One of our constructive suggestions, to Infovis practitioners and statisticians alike, is to try not to cram into a single graph what can be better displayed in two or more. We recognize that we offer only one perspective and intend this article to be a starting point for a wide-ranging discussion among graphic designers, statisticians, and users of statistical methods. The purpose of this article is not to criticize but to explore the different goals that lead researchers in different fields to value different aspects of data visualization.},
  Doi                      = {10.1080/10618600.2012.761137},
  File                     = {Full Text PDF:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/FPZGWWSB/Gelman and Unwin - 2013 - Infovis and Statistical Graphics Different Goals,.pdf:application/pdf;Snapshot:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/JD3PKJ4X/10618600.2012.html:text/html},
  ISSN                     = {1061-8600},
  Shorttitle               = {Infovis and {Statistical} {Graphics}},
  Url                      = {http://www.tandfonline.com/doi/abs/10.1080/10618600.2012.761137},
  Urldate                  = {2013-07-01}
}

@Article{gelman_tradeoffs_2013,
  Title                    = {Tradeoffs in {Information} {Graphics}},
  Author                   = {Gelman, Andrew and Unwin, Antony},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {45--49},
  Volume                   = {22},

  Doi                      = {10.1080/10618600.2012.761141},
  ISSN                     = {1061-8600},
  Url                      = {http://www.tandfonline.com/doi/abs/10.1080/10618600.2012.761141},
  Urldate                  = {2013-06-03}
}

@Article{goldberg_bivariate_1992,
  Title                    = {Bivariate {Extensions} of the {Boxplot}},
  Author                   = {Goldberg, Kenneth M. and Iglewicz, Boris},
  Journal                  = {Technometrics},
  Year                     = {1992},

  Month                    = aug,
  Number                   = {3},
  Pages                    = {307--320},
  Volume                   = {34},

  Abstract                 = {The boxplot has proven to be a very useful tool for summarizing univariate data. Several options of bivariate boxplot-type constructions are discussed. These include both elliptic and asymmetric plots. An inner region contains 50\% of the data, and a fence identifies potential outliers. Such a robust plot shows location, scale, correlation, and a resistant regression line. Alternative constructions are compared in terms of efficiency of the relevant parameters. Additional properties are given and recommendations made. Emphasis is given to the bivariate biweight M estimator. Several practical examples illustrate that standard least squares ellipsoids can give graphically misleading summaries.},
  Doi                      = {10.1080/00401706.1992.10485280},
  ISSN                     = {0040-1706},
  Keywords                 = {Ellipsoid, Outlier, Resistant line, Robust estimators},
  Url                      = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1992.10485280},
  Urldate                  = {2019-09-11}
}

@InProceedings{heer_crowdsourcing_2010,
  Title                    = {Crowdsourcing graphical perception: using mechanical turk to assess visualization design},
  Author                   = {Heer, Jeffrey and Bostock, Michael},
  Booktitle                = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
  Year                     = {2010},
  Pages                    = {203--212},
  Publisher                = {ACM},

  Shorttitle               = {Crowdsourcing graphical perception}
}

@Article{hosmer_comparison_1997,
  Title                    = {A comparison of goodness-of-fit tests for the logistic regression model},
  Author                   = {Hosmer, David W. and Hosmer, Trina and Le Cessie, Saskia and Lemeshow, Stanley},
  Journal                  = {Statistics in medicine},
  Year                     = {1997},
  Number                   = {9},
  Pages                    = {965--980},
  Volume                   = {16}
}

@Book{huff_how_1993,
  Title                    = {How to lie with statistics},
  Author                   = {Huff, Darrell},
  Publisher                = {WW Norton \& Company},
  Year                     = {1993},
  Note                     = {originally published 1954}
}

@Misc{hullman_how_2019,
  Title                    = {How to {Get} {Better} at {Embracing} {Unknowns}: {Interpreting} uncertainty through data visualizations},

  Author                   = {Hullman, Jessica},
  Month                    = sep,
  Year                     = {2019},

  Abstract                 = {Data-visualization techniques can clarify the uncertainty in information or make it more confusing if not implemented well.
 For example, even though error bars seem exact, people often misunderstand them. 
 Quantile dot plots can be effective because they present uncertainty as discrete probabilities that people can readily see. 
 Effective visualizations of uncertainty can help us make judgments, analytically and emotionally, about the likelihood of future events.},
  Journal                  = {Scientific American},
  Url                      = {https://www.scientificamerican.com/article/how-to-get-better-at-embracing-unknowns/},
  Urldate                  = {2019-09-03}
}

@Misc{irizarry_you_2019,
  Title                    = {You can replicate almost any plot with {R} · {Simply} {Statistics}},

  Author                   = {Irizarry, Rafael A.},
  Month                    = aug,
  Year                     = {2019},

  Abstract                 = {Although R is great for quickly turning data into plots, it is not widely used for making publication ready figures. But, with enough tinkering you can make almost any plot in R.},
  Journal                  = {Simply Statistics},
  Url                      = {https://simplystatistics.org/2019/08/28/you-can-replicate-almost-any-plot-with-ggplot2/},
  Urldate                  = {2019-08-30}
}

@Misc{johnston_ggplot_2015,
  Title                    = {ggplot fish},

  Author                   = {Johnston, Myfanwy},
  Month                    = mar,
  Year                     = {2015},

  Abstract                 = {guys. GUYS. I'm diving in Palau this week and I've found the \#ggplot2 fish. \#rstats},
  Journal                  = {Twitter},
  Language                 = {en},
  Shorttitle               = {ggplot fish},
  Url                      = {https://twitter.com/voovarb/status/580722323843596288/photo/1},
  Urldate                  = {2019-09-10}
}

@Article{kahle_ggmap_2013,
  Title                    = {ggmap : {Spatial} {Visualization} with ggplot2},
  Author                   = {Kahle, David and Wickham, Hadley},
  Journal                  = {The R Journal},
  Year                     = {2013},

  Month                    = jun,
  Number                   = {1},
  Pages                    = {144--162},
  Volume                   = {5},

  Url                      = {http://journal.r-project.org/archive/2013-1/RJournal_2013-1_kahle-wickham.pdf}
}

@Article{kosara_infovis_2013,
  Title                    = {{InfoVis} {Is} {So} {Much} {More}: {A} {Comment} on {Gelman} and {Unwin} and an {Invitation} to {Consider} the {Opportunities}},
  Author                   = {Kosara, Robert},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2013},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {29--32},
  Volume                   = {22},

  Doi                      = {10.1080/10618600.2012.755465},
  File                     = {Snapshot:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/UTYES3JA/10618600.2012.html:text/html},
  ISSN                     = {1061-8600},
  Shorttitle               = {{InfoVis} {Is} {So} {Much} {More}},
  Url                      = {https://doi.org/10.1080/10618600.2012.755465},
  Urldate                  = {2018-02-12}
}

@Article{levine_editorial_2010,
  Title                    = {Editorial: {Publishing} {Animations}, 3D {Visualizations}, and {Movies} in {JCGS}},
  Author                   = {Levine, Richard A. and Tierney, Luke and Wickham, Hadley and Sampson, Eric and Cook, Dianne and van Dyk, David A.},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2010},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {1--2},
  Volume                   = {19},

  Doi                      = {10.1198/jcgs.2010.191ed},
  ISSN                     = {1061-8600, 1537-2715},
  Shorttitle               = {Editorial},
  Url                      = {http://amstat.tandfonline.com/doi/abs/10.1198/jcgs.2010.191ed},
  Urldate                  = {2012-05-11}
}

@Article{li_judging_2008,
  Title                    = {Judging correlation from scatterplots and parallel coordinate plots},
  Author                   = {Li, Jing and Martens, Jean-Bernard and van Wijk, Jarke J},
  Journal                  = {Information Visualization},
  Year                     = {2008},

  Month                    = may,

  Doi                      = {10.1057/palgrave.ivs.9500179},
  File                     = {Information Visualization - Abstract of article\: Judging correlation from scatterplots and parallel coordinate plots:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/IPH4KZVV/9500179a.html:text/html},
  ISSN                     = {1473-8716},
  Url                      = {http://www.palgrave-journals.com/ivs/journal/vaop/ncurrent/abs/9500179a.html},
  Urldate                  = {2009-11-11}
}

@Misc{lindberg_viz_2017,
  Title                    = {viz-pub: {A} place to publish data-vizes},

  Author                   = {Lindberg, Henrik},
  Month                    = nov,
  Note                     = {original-date: 2017-06-08T08:40:42Z},
  Year                     = {2017},

  File                     = {Snapshot:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/T4J3DPJF/viz-pub.html:text/html},
  Shorttitle               = {viz-pub},
  Url                      = {https://github.com/halhen/viz-pub},
  Urldate                  = {2017-11-07}
}

@Misc{machlis_best_2015,
  Title                    = {Best {R} packages for data import, data wrangling \& data visualization},

  Author                   = {Machlis, Sharon},
  Month                    = nov,
  Year                     = {2015},

  Abstract                 = {Useful R packages in a handy searchable table},
  File                     = {Snapshot:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/8B6B4ZUG/great-r-packages-for-data-import-wrangling-visualization.html:text/html},
  Journal                  = {Computerworld},
  Url                      = {http://www.computerworld.com/article/2921176/business-intelligence/great-r-packages-for-data-import-wrangling-visualization.html},
  Urldate                  = {2016-05-16}
}

@Article{mccallum_situ_2017,
  Title                    = {In situ exposure to wastewater effluent reduces survival but has little effect on the behaviour or physiology of an invasive {Great} {Lakes} fish},
  Author                   = {McCallum, Erin S. and Du, Sherry N. N. and Vaseghi-Shanjani, Maryam and Choi, Jasmine A. and Warriner, Theresa R. and Sultana, Tamanna and Scott, Graham R. and Balshine, Sigal},
  Journal                  = {Aquatic Toxicology},
  Year                     = {2017},

  Month                    = mar,
  Pages                    = {37--48},
  Volume                   = {184},

  Abstract                 = {Treated effluents from wastewater treatment plants (WWTP) are a significant source of anthropogenic contaminants, such as pharmaceuticals, in the aquatic environment. Although our understanding of how wastewater effluent impacts fish reproduction is growing, we know very little about how effluent affects non-reproductive physiology and behaviours associated with fitness (such as aggression and activity). To better understand how fish cope with chronic exposure to wastewater effluent in the wild, we caged round goby (Neogobius melanostomus) for three weeks at different distances from a wastewater outflow. We evaluated the effects of this exposure on fish survival, behaviour, metabolism, and respiratory traits. Fish caged inside the WWTP and close to the outfall experienced higher mortality than fish from the reference site. Interestingly, those fish that survived the exposure performed similarly to fish caged at the reference site in tests of aggressive behaviour, startle-responses, and dispersal. Moreover, the fish near WWTP outflow displayed similar resting metabolism (O2 consumption rates), hypoxia tolerance, haemoglobin concentration, haematocrit, and blood-oxygen binding affinities as the fish from the more distant reference site. We discuss our findings in relation to exposure site water quality, concentrations of pharmaceutical and personal care product pollutants, and our test species tolerance.},
  Doi                      = {10.1016/j.aquatox.2016.12.017},
  ISSN                     = {0166-445X},
  Keywords                 = {Round goby, Activity, Caging, Cootes Paradise Marsh, PPCPs, Respirometry},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0166445X16303757},
  Urldate                  = {2018-02-06}
}

@Article{mcgill_variations_1978,
  Title                    = {Variations of {Box} {Plots}},
  Author                   = {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
  Journal                  = {The American Statistician},
  Year                     = {1978},
  Number                   = {1},
  Pages                    = {12--16},
  Volume                   = {32},

  Abstract                 = {Box plots display batches of data. Five values from a set of data are conventionally used; the extremes, the upper and lower hinges (quartiles), and the median. Such plots are becoming a widely used tool in exploratory data analysis and in preparing visual summaries for statisticians and nonstatisticians alike. Three variants of the basic display, devised by the authors, are described. The first visually incorporates a measure of group size; the second incorporates an indication of rough significance of differences between medians; the third combines the features of the first two. These techniques are displayed by examples.},
  Doi                      = {10.2307/2683468},
  ISSN                     = {0003-1305},
  Url                      = {http://www.jstor.org/stable/2683468},
  Urldate                  = {2018-01-10}
}

@Misc{meeks_color_2018,
  Title                    = {Color {Advice} for {Data} {Visualization} with {D}3.js},

  Author                   = {Meeks, Elijah},
  Month                    = jan,
  Year                     = {2018},

  Abstract                 = {Using color in a post-category20 world},
  Journal                  = {Elijah Meeks},
  Url                      = {https://medium.com/@Elijah_Meeks/color-advice-for-data-visualization-with-d3-js-33b5adc41c90},
  Urldate                  = {2018-02-02}
}

@Book{meeks_d3_2017,
  Title                    = {D3.js in Action: Data visualization with JavaScript},
  Author                   = {Elijah Meeks},
  Publisher                = {Manning},
  Year                     = {2017},
  Edition                  = {2},

  ISBN                     = {9781617294488},
  Url                      = {https://www.manning.com/books/d3js-in-action-second-edition}
}

@Article{murrell_infovis_2013,
  Title                    = {{InfoVis} and {Statistical} {Graphics}: {Comment}},
  Author                   = {Murrell, Paul},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2013},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {33--37},
  Volume                   = {22},

  Doi                      = {10.1080/10618600.2012.751875},
  File                     = {Snapshot:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/RPZE7DQ8/10618600.2012.html:text/html},
  ISSN                     = {1061-8600},
  Shorttitle               = {{InfoVis} and {Statistical} {Graphics}},
  Url                      = {https://doi.org/10.1080/10618600.2012.751875},
  Urldate                  = {2018-02-12}
}

@Misc{noren_cost_2011,
  Title                    = {Cost of {Health} {Care} by {Country}},

  Author                   = {Noren, Laura},
  Month                    = apr,
  Year                     = {2011},

  Abstract                 = {The Society Pages (TSP) is an open-access social science project headquartered in the Department of Sociology at the University of Minnesota},
  Journal                  = {Graphic Sociology},
  Language                 = {en},
  Url                      = {https://thesocietypages.org/graphicsociology/2011/04/26/cost-of-health-care-by-country-national-geographic/},
  Urldate                  = {2018-02-12}
}

@Article{nunez_optimizing_2017,
  Title                    = {Optimizing colormaps with consideration for color vision deficiency to enable accurate interpretation of scientific data},
  Author                   = {Nuñez, Jamie R. and Anderton, Chris R. and Renslow, Ryan S.},
  Journal                  = {arXiv preprint arXiv:1712.01662},
  Year                     = {2017}
}

@Article{peltonen_visualizations_2009,
  Title                    = {Visualizations for assessing convergence and mixing of {Markov} chain {Monte} {Carlo} simulations},
  Author                   = {Peltonen, Jaakko and Venna, Jarkko and Kaski, Samuel},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2009},

  Month                    = oct,
  Number                   = {12},
  Pages                    = {4453--4470},
  Volume                   = {53},

  Doi                      = {10.1016/j.csda.2009.07.001},
  ISSN                     = {01679473},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S0167947309002461}
}

@Article{peltonen_visualizations_2009,
  Title                    = {Visualizations for assessing convergence and mixing of {Markov} chain {Monte} {Carlo} simulations},
  Author                   = {Peltonen, Jaakko and Venna, Jarkko and Kaski, Samuel},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2009},

  Month                    = oct,
  Number                   = {12},
  Pages                    = {4453--4470},
  Volume                   = {53},

  Doi                      = {10.1016/j.csda.2009.07.001},
  ISSN                     = {01679473},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S0167947309002461}
}

@Book{pugin_true_1853,
  Title                    = {The true principles of pointed or {Christian} architecture: set forth in two lectures delivered at {St}. {Marie}'s, {Oscott}},
  Author                   = {Pugin, Augustus Welby Northmore},
  Publisher                = {H. G. Bohn},
  Year                     = {1853},
  Note                     = {Google-Books-ID: QphZAAAAYAAJ},

  Keywords                 = {Architecture / General, Architecture / History / General, Architecture / Religious Buildings, Architecture, Gothic},
  Language                 = {en},
  Shorttitle               = {The true principles of pointed or {Christian} architecture},
  Url                      = {https://books.google.ca/books?id=QphZAAAAYAAJ}
}

@Book{QuinnKeough2002,
  Title                    = {Experimental Design and Data Analysis for Biologists},
  Author                   = {Gerry P. Quinn and Michael J. Keough},
  Publisher                = {Cambridge University Press},
  Year                     = {2002},

  Address                  = {Cambridge, England},

  ISBN                     = {0521009766}
}

@Misc{rauser_how_2016,
  Title                    = {How {Humans} {See} {Data}},

  Author                   = {Rauser, John},
  Month                    = dec,
  Year                     = {2016},

  Abstract                 = {John Rauser explains a few of the most important results from research into the functioning of the human visual system and the question of how humans decode information presented in graphical form. By understanding and applying this research when designing statistical graphics, you can simplify difficult analytical tasks as much as possible.},
  Keywords                 = {data, PSYCHOLOGY, statistics, Visualization, graphs, monitoring, velocity},
  Url                      = {https://www.youtube.com/watch?v=fSgEeI2Xpdc},
  Urldate                  = {2017-07-24}
}

@Article{rougier_ten_2014,
  Title                    = {Ten {Simple} {Rules} for {Better} {Figures}},
  Author                   = {Rougier, Nicolas P. and Droettboom, Michael and Bourne, Philip E.},
  Journal                  = {PLOS Computational Biology},
  Year                     = {2014},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {e1003833},
  Volume                   = {10},

  Doi                      = {10.1371/journal.pcbi.1003833},
  ISSN                     = {1553-7358},
  Keywords                 = {data visualization, Research Design, Software tools, Vision, Software design, Radii, Eye movements, Seismic signal processing},
  Url                      = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003833},
  Urldate                  = {2016-12-21}
}

@Article{rousseeuw_bagplot_1999,
  Title                    = {The {Bagplot}: {A} {Bivariate} {Boxplot}},
  Author                   = {Rousseeuw, Peter J. and Ruts, Ida and Tukey, John W.},
  Journal                  = {The American Statistician},
  Year                     = {1999},

  Month                    = nov,
  Number                   = {4},
  Pages                    = {382--387},
  Volume                   = {53},

  Abstract                 = {We propose the bagplot, a bivariate generalization of the univariate boxplot. The key notion is the half space location depth of a point relative to a bivariate dataset, which extends the univariate concept of rank. The “depth median” is the deepest location, and it is surrounded by a “bag” containing the n/2 observations with largest depth. Magnifying the bag by a factor 3 yields the “fence” (which is not plotted). Observations between the bag and the fence are marked by a light gray loop, whereas observations outside the fence are flagged as outliers. The bagplot visualizes the location, spread, correlation, skewness, and tails of the data. It is equivariant for linear transformations, and not limited to elliptical distributions. Software for drawing the bagplot is made available for the S-Plus and MATLAB environments. The bagplot is illustrated on several datasets—for example, in a scatterplot matrix of multivariate data.},
  Doi                      = {10.1080/00031305.1999.10474494},
  ISSN                     = {0003-1305},
  Keywords                 = {Algorithms, Depth contours, Graphical display, Location depth, Ranks},
  Shorttitle               = {The {Bagplot}},
  Urldate                  = {2019-09-11}
}

@Book{sarkar_lattice_2008,
  Title                    = {Lattice: {Multivariate} {Data} {Visualization} with {R}},
  Author                   = {Sarkar, Deepayan},
  Publisher                = {Springer},
  Year                     = {2008},
  Edition                  = {1},
  Month                    = mar,

  ISBN                     = {0-387-75968-9},
  Shorttitle               = {Lattice}
}

@Article{schielzeth_simple_2010,
  Title                    = {Simple means to improve the interpretability of regression coefficients},
  Author                   = {Schielzeth, Holger},
  Journal                  = {Methods in Ecology and Evolution},
  Year                     = {2010},
  Pages                    = {103-113},
  Volume                   = {1},

  Abstract                 = {1. Linear regression models are an important statistical tool in evolutionary and ecological studies. Unfortunately, these models often yield some uninterpretable estimates and hypothesis tests, especially when models contain interactions or polynomial terms. Furthermore, the standard errors for treatment groups, although often of interest for including in a publication, are not directly available in a standard linear model. 2. Centring and standardization of input variables are simple means to improve the interpretability of regression coefficients. Further, refitting the model with a slightly modified model structure allows extracting the appropriate standard errors for treatment groups directly from the model. 3. Centring will make main effects biologically interpretable even when involved in interactions and thus avoids the potential misinterpretation of main effects. This also applies to the estimation of linear effects in the presence of polynomials. Categorical input variables can also be centred and this sometimes assists interpretation. 4. Standardization (z-transformation) of input variables results in the estimation of standardized slopes or standardized partial regression coefficients. Standardized slopes are comparable in magnitude within models as well as between studies. They have some advantages over partial correlation coefficients and are often the more interesting standardized effect size. 5. The thoughtful removal of intercepts or main effects allows extracting treatment means or treatment slopes and their appropriate standard errors directly from a linear model. This provides a simple alternative to the more complicated calculation of standard errors from contrasts and main effects. 6. The simple methods presented here put the focus on parameter estimation (point estimates as well as confidence intervals) rather than on significance thresholds. They allow fitting complex, but meaningful models that can be concisely presented and interpreted. The presented methods can also be applied to generalised linear models {(GLM)} and linear mixed models.},
  Doi                      = {10.1111/j.2041-210X.2010.00012.x},
  Url                      = {http://dx.doi.org/10.1111/j.2041-210X.2010.00012.x}
}

@Article{schultz_evidence_2016,
  Title                    = {Evidence for a trophic cascade on rocky reefs following sea star mass mortality in {British} {Columbia}},
  Author                   = {Schultz, Jessica A. and Cloutier, Ryan N. and Côté, Isabelle M.},
  Journal                  = {PeerJ},
  Year                     = {2016},

  Month                    = apr,
  Pages                    = {e1980},
  Volume                   = {4},

  Abstract                 = {Echinoderm population collapses, driven by disease outbreaks and climatic events, may be important drivers of population dynamics, ecological shifts and biodiversity. The northeast Pacific recently experienced a mass mortality of sea stars. In Howe Sound, British Columbia, the sunflower star Pycnopodia helianthoides—a previously abundant predator of bottom-dwelling invertebrates—began to show signs of a wasting syndrome in early September 2013, and dense aggregations disappeared from many sites in a matter of weeks. Here, we assess changes in subtidal community composition by comparing the abundance of fish, invertebrates and macroalgae at 20 sites in Howe Sound before and after the 2013 sea star mortality to evaluate evidence for a trophic cascade. We observed changes in the abundance of several species after the sea star mortality, most notably a four-fold increase in the number of green sea urchins, Strongylocentrotus droebachiensis, and a significant decline in kelp cover, which are together consistent with a trophic cascade. Qualitative data on the abundance of sunflower stars and green urchins from a citizen science database show that the patterns of echinoderm abundance detected at our study sites reflected wider local trends. The trophic cascade evident at the scale of Howe Sound was observed at half of the study sites. It remains unclear whether the urchin response was triggered directly, via a reduction in urchin mortality, or indirectly, via a shift in urchin distribution into areas previously occupied by the predatory sea stars. Understanding the ecological implications of sudden and extreme population declines may further elucidate the role of echinoderms in temperate seas, and provide insight into the resilience of marine ecosystems to biological disturbances.},
  Doi                      = {10.7717/peerj.1980},
  ISSN                     = {2167-8359},
  Language                 = {en},
  Url                      = {https://peerj.com/articles/1980},
  Urldate                  = {2018-02-06}
}

@Misc{sciani_cividis_2018,
  Title                    = {cividis: {Implementation} of the {Matplotlib} 'viridis' color map in {R} (lite version)},

  Author                   = {Sciani, Marco},
  Month                    = jan,
  Note                     = {original-date: 2018-01-15T07:56:06Z},
  Year                     = {2018},

  Shorttitle               = {cividis},
  Url                      = {https://github.com/marcosci/cividis},
  Urldate                  = {2018-01-15}
}

@Book{steele_beautiful_2010,
  Title                    = {Beautiful {Visualization}: {Looking} at {Data} through the {Eyes} of {Experts}},
  Author                   = {Steele, Julie and Iliinsky, Noah},
  Publisher                = {O'Reilly Media},
  Year                     = {2010},
  Edition                  = {1},
  Month                    = jun,

  ISBN                     = {1-4493-7986-9},
  Shorttitle               = {Beautiful {Visualization}}
}

@Article{su_its_2008,
  Title                    = {It’s easy to produce chartjunk using {Microsoft}®{Excel} 2007 but hard to make good graphs},
  Author                   = {Su, Yu-Sung},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2008},

  Month                    = jun,
  Number                   = {10},
  Pages                    = {4594--4601},
  Volume                   = {52},

  Abstract                 = {The purpose of default settings in a graphic tool is to make it easy to produce good graphics that accord with the principles of statistical graphics, e.g., [Tufte, E.R., 1990. Envisioning Information. Graphics Press, Cheshire, Conn, Tufte, E.R., 1997. Visual Explanations: Images and Quantities, Evidence and Narrative, 2nd Edition. Graphics Press, Cheshire, Conn, Cleveland, W.S., 1993. Visualizing Data. Hobart Press, N.J Cleveland, W.S., 1994. The Elements of Graphing Data, rev. edition. AT\&T Bell Laboratories, Murray Hill, N.J, Wainer, H., 1997. Visual revelations: Graphical tales of fate and deception from Napoleon to Ross Perot. Copernicus, New York, Spence, R., 2001. Information Visualization. ACM Press \& AddisonWesley, New York, and Few, S., 2004. Show Me the Numbers. Analytic Press, Hillsdale, NJ]. If the defaults do not embody these principles, then the only way to produce good graphics is to be sufficiently familiar with the principles of statistical graphics. This paper shows that Excel graphics defaults do not embody the appropriate principles. Users who want to use Excel are advised to know the principles of good graphics well enough so that they can choose the appropriate options to override the defaults. Microsoft® should overhaul the Excel graphics engine so that its defaults embody the principles of statistical graphics and make it easy for non-experts to produce good graphs.},
  Doi                      = {10.1016/j.csda.2008.03.007},
  File                     = {ScienceDirect Snapshot:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/T6N4TMUQ/science.html:text/html},
  Url                      = {http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V8V-4S1S6FC-6&_user=10&_rdoc=1&_fmt=&_orig=search&_sort=d&view=c&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=d3e57dcf23c1adccee012110b71fefcf},
  Urldate                  = {2008-07-04}
}

@InProceedings{swayne_exploratory_2004,
  Title                    = {Exploratory visual analysis of graphs in {GGobi}},
  Author                   = {Swayne, Deborah F. and Buja, Andreas and Lang, Duncan Temple},
  Booktitle                = {Proceeds of the 3d International Workshop on Distributed Statistical Computing (DSC 2003)},
  Year                     = {2004},
  Editor                   = {K. Hornik and F. Leisch},
  Pages                    = {477--488},
  Publisher                = {Springer},

  Url                      = {https://link.springer.com/chapter/10.1007/978-3-7908-2656-2_39}
}

@Article{thompson_graphical_2010,
  Title                    = {Graphical {Comparison} of {MCMC} {Performance}},
  Author                   = {Thompson, Madeleine B},
  Journal                  = {1011.4457},
  Year                     = {2010},

  Month                    = nov,

  Abstract                 = {This paper presents a graphical method for comparing performance of Markov Chain Monte Carlo methods. Most researchers present comparisons of MCMC methods using tables of figures of merit; this paper presents a graphical alternative. It first discusses the computation of autocorrelation time, then uses this to construct a figure of merit, log density function evaluations per independent observation. Then, it demonstrates how one can plot this figure of merit against a tuning parameter in a grid of plots where columns represent sampling methods and rows represent distributions. This type of visualization makes it possible to convey a greater depth of information without overwhelming the user with numbers, allowing researchers to put their contributions into a broader context than is possible with a textual presentation.},
  File                     = {1011.4457 PDF:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/VHPNUKD5/Thompson - 2010 - Graphical Comparison of MCMC Performance.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/Q2R69HTJ/1011.html:text/html},
  Keywords                 = {Statistics - Computation, G.3},
  Url                      = {http://arxiv.org/abs/1011.4457},
  Urldate                  = {2010-11-22}
}

@Article{tibshirani_estimating_1987,
  Title                    = {Estimating optimal transformations for regression},
  Author                   = {Rob Tibshirani},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1987},
  Pages                    = {394},
  Volume                   = {83}
}

@Book{tufte_visual_2001,
  Title                    = {The {Visual} {Display} of {Quantitative} {Information}},
  Author                   = {Tufte, Edward},
  Publisher                = {Graphics Press},
  Year                     = {2001},
  Edition                  = {2d}
}

@Book{tufte_beautiful_2006,
  Title                    = {Beautiful evidence},
  Author                   = {Tufte, Edward R},
  Publisher                = {Graphics Press},
  Year                     = {2006},

  Address                  = {Cheshire, Conn.},

  ISBN                     = {0-9613921-7-7 978-0-9613921-7-8},
  Language                 = {English}
}

@Book{tufte_visual_1997,
  Title                    = {Visual explanations: images and quantities, evidence and narrative},
  Author                   = {Tufte, Edward R},
  Publisher                = {Graphics Press},
  Year                     = {1997},

  Address                  = {Cheshire, Conn.},

  ISBN                     = {0-9613921-2-6 978-0-9613921-2-3},
  Language                 = {English},
  Shorttitle               = {Visual explanations}
}

@Book{tufte_envisioning_1995,
  Title                    = {Envisioning information},
  Author                   = {Tufte, Edward R},
  Publisher                = {Graphics Press},
  Year                     = {1995},

  Address                  = {Cheshire, Conn.},

  ISBN                     = {0-9613921-1-8 978-0-9613921-1-6},
  Language                 = {English}
}

@Article{tufte_dataink_1990,
  Title                    = {Data-{Ink} {Maximization} and {Graphical} {Design}},
  Author                   = {Tufte, E. R.},
  Journal                  = {Oikos},
  Year                     = {1990},

  Month                    = jun,
  Note                     = {ArticleType: research-article / Full publication date: Jun., 1990 / Copyright © 1990 Nordic Society Oikos},
  Number                   = {2},
  Pages                    = {130--144},
  Volume                   = {58},

  Copyright                = {Copyright © 1990 Nordic Society Oikos},
  Doi                      = {10.2307/3545420},
  ISSN                     = {0030-1299},
  Url                      = {http://www.jstor.org/stable/3545420},
  Urldate                  = {2013-01-03}
}

@PhdThesis{wei_extending_2017,
  Title                    = {Extending {Growth} {Mixture} {Models} and {Handling} {Missing} {Values} via {Mixtures} of {Non}-{Elliptical} {Distributions}},
  Author                   = {Wei, Yuhong},
  Year                     = {2017},
  Type                     = {Thesis},

  Abstract                 = {Growth mixture models (GMMs) are used to model intra-individual change and inter-individual differences in change and to detect underlying group structure in longitudinal studies. Regularly, these models are fitted under the assumption of normality, an assumption that is frequently invalid. To this end, this thesis focuses on the development of novel non-elliptical growth mixture models to better fit real data. Two non-elliptical growth mixture models, via the multivariate skew-t distribution and the generalized hyperbolic distribution, are developed and applied to simulated and real data. Furthermore, these two non-elliptical growth mixture models are extended to accommodate missing values, which are near-ubiquitous in real data.

Recently, finite mixtures of non-elliptical distributions have flourished and facilitated the flexible clustering of the data featuring longer tails and asymmetry. However, in practice, real data often have missing values, and so work in this direction is also pursued. A novel approach, via mixtures of the generalized hyperbolic distribution and mixtures of the multivariate skew-t distributions, is presented to handle missing values in mixture model-based clustering context. To increase parsimony, families of mixture models have been developed by imposing constraints on the component scale matrices whenever missing data occur. Next, a mixture of generalized hyperbolic factor analyzers model is also proposed to cluster high-dimensional data with different patterns of missing values. Two missingness indicator matrices are also introduced to ease the computational burden. The algorithms used for parameter estimation are presented, and the performance of the methods is illustrated on simulated and real data.},
  Language                 = {en},
  Url                      = {https://macsphere.mcmaster.ca/handle/11375/21987},
  Urldate                  = {2018-02-04}
}

@Book{wickham_ggplot2_2009,
  Title                    = {ggplot2: {Elegant} {Graphics} for {Data} {Analysis}},
  Author                   = {Wickham, Hadley},
  Publisher                = {Springer},
  Year                     = {2009},
  Edition                  = {2nd Printing.},
  Month                    = aug,

  ISBN                     = {0-387-98140-3},
  Shorttitle               = {ggplot2}
}

@Article{wickham_graphical_2010,
  Title                    = {Graphical inference for infovis},
  Author                   = {Wickham, H. and Cook, D. and Hofmann, H. and Buja, Andreas},
  Journal                  = {IEEE Transactions on Visualization and Computer Graphics},
  Year                     = {2010},

  Month                    = nov,
  Number                   = {6},
  Pages                    = {973--979},
  Volume                   = {16},

  Abstract                 = {How do we know if what we see is really there? When visualizing data, how do we avoid falling into the trap of apophenia where we see patterns in random noise? Traditionally, infovis has been concerned with discovering new relationships, and statistics with preventing spurious relationships from being reported. We pull these opposing poles closer with two new techniques for rigorous statistical inference of visual discoveries. The "Rorschach" helps the analyst calibrate their understanding of uncertainty and "line-up" provides a protocol for assessing the significance of visual discoveries, protecting against the discovery of spurious structure.},
  Doi                      = {10.1109/TVCG.2010.161},
  File                     = {IEEE Xplore Abstract Record:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/6U3IB5QM/abs_all.html:text/html;Wickham et al_2010_Graphical inference for infovis.pdf:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/KGAEDMF8/Wickham et al_2010_Graphical inference for infovis.pdf:application/pdf},
  ISSN                     = {1077-2626},
  Keywords                 = {Testing, Models, Statistical, Neoplasms, null hypotheses, Computer Graphics, infovis, data visualization, visual testing, Protocols, Accuracy, data visualisation, Humans, Databases, Factual, Histograms, data plot, Data Interpretation, Statistical, graphical inference, Tag clouds, statistics, permutation tests, Visualization, Statistical inference}
}

@TechReport{boxplots,
  Title                    = {40 years of boxplots},
  Author                   = {Hadley Wickham and Lisa Stryjewski},
  Institution              = {had.co.nz},
  Year                     = {2012},

  Url                      = {http://vita.had.co.nz/papers/boxplots.pdf}
}

@Book{Wilkinson1999,
  Title                    = {The grammar of graphics},
  Author                   = {L. Wilkinson},
  Publisher                = {Springer},
  Year                     = {1999},

  Address                  = {New York}
}

@Article{you_heres_2017,
  Title                    = {Here’s the visual proof of why vaccines do more good than harm},
  Author                   = {Jia You},
  Journal                  = {Science},
  Year                     = {2017},

  Month                    = apr,

  Abstract                 = {Vaccines have beat back infectious diseases. Bubbles represent reported U.S. cases, but not all diseases were notifiable in all years. For example, mumps was not reported until 1968, the year after a vaccine was licensed. Click or hover on a bubble or a dot on the timeline to view more details.},
  Url                      = {http://www.sciencemag.org/news/2017/04/here-s-visual-proof-why-vaccines-do-more-good-harm}
}

@Misc{zeileis_was_2019,
  Title                    = {Was {Trump} confused by rainbow color map?},

  Author                   = {Zeileis, Achim},
  Month                    = sep,
  Year                     = {2019},

  Abstract                 = {In the controversy over Hurricane Dorian, US President Donald Trump repeatedly claimed that early forecasts showed a high probability of Alabama being hit. We demonstrate that a potentially confusing rainbow color map in the official weather forecasts may have attributed to Trump overestimating the risk.},
  Journal                  = {Achim Zeileis},
  Language                 = {en},
  Url                      = {https://eeecon.uibk.ac.at/~zeileis//news/dorian_rainbow/},
  Urldate                  = {2019-09-09}
}

@Article{zeileis_object_2006,
  Title                    = {Object-Oriented Computation of Sandwich Estimators},
  Author                   = {Achim Zeileis},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2006},
  Number                   = {9},
  Pages                    = {1-16},
  Volume                   = {16},

  Url                      = {http://www.jstatsoft.org/v16/i09/}
}

@article{zeileis_escaping_2009,
  Title                    = {Escaping {RGBland}: {Selecting} colors for statistical graphics},
  Author                   = {Zeileis, Achim and Hornik, Kurt and Murrell, Paul},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2009},

  Month                    = jul,
  Number                   = {9},
  Pages                    = {3259--3270},
  Volume                   = {53},

  Abstract                 = {Statistical graphics are often augmented by the use of color coding information contained in some variable. When this involves the shading of areas (and not only points or lines)—e.g., as in bar plots, pie charts, mosaic displays or heatmaps—it is important that the colors are perceptually based and do not introduce optical illusions or systematic bias. Based on the perceptually-based Hue-Chroma-Luminance (HCL) color space suitable color palettes are derived for coding categorical data (qualitative palettes) and numerical variables (sequential and diverging palettes).},
  Doi                      = {10.1016/j.csda.2008.11.033},
  ISSN                     = {0167-9473},
  Shorttitle               = {Escaping {RGBland}},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167947308005549},
  Urldate                  = {2019-09-11}
}


@Article{zuur_protocol_2016,
  Title                    = {A protocol for conducting and presenting results of regression-type analyses},
  Author                   = {Zuur, Alain F. and Ieno, Elena N.},
  Journal                  = {Methods in Ecology and Evolution},
  Year                     = {2016},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {636--645},
  Volume                   = {7},

  Abstract                 = {* Scientific investigation is of value only insofar as relevant results are obtained and communicated, a task that requires organizing, evaluating, analysing and unambiguously communicating the significance of data. In this context, working with ecological data, reflecting the complexities and interactions of the natural world, can be a challenge. Recent innovations for statistical analysis of multifaceted interrelated data make obtaining more accurate and meaningful results possible, but key decisions of the analyses to use, and which components to present in a scientific paper or report, may be overwhelming.


* We offer a 10-step protocol to streamline analysis of data that will enhance understanding of the data, the statistical models and the results, and optimize communication with the reader with respect to both the procedure and the outcomes. The protocol takes the investigator from study design and organization of data (formulating relevant questions, visualizing data collection, data exploration, identifying dependency), through conducting analysis (presenting, fitting and validating the model) and presenting output (numerically and visually), to extending the model via simulation. Each step includes procedures to clarify aspects of the data that affect statistical analysis, as well as guidelines for written presentation. Steps are illustrated with examples using data from the literature.


* Following this protocol will reduce the organization, analysis and presentation of what may be an overwhelming information avalanche into sequential and, more to the point, manageable, steps. It provides guidelines for selecting optimal statistical tools to assess data relevance and significance, for choosing aspects of the analysis to include in a published report and for clearly communicating information.},
  Doi                      = {10.1111/2041-210X.12577},
  ISSN                     = {2041-210X},
  Keywords                 = {Visualization, effective communication, protocol, statistical analysis},
  Language                 = {en},
  Url                      = {http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12577/abstract},
  Urldate                  = {2016-06-17}
}

@Misc{noauthor_data_nodate,
  Title                    = {Data visualization: {A} view of every {Points} of {View} column : {Methagora}},

  File                     = {Data visualization\: A view of every Points of View column \: Methagora:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/24IZIWDV/data-visualization-points-of-view.html:text/html},
  Url                      = {http://blogs.nature.com/methagora/2013/07/data-visualization-points-of-view.html},
  Urldate                  = {2015-05-22}
}

@Book{noauthor_data_nodate-1,
  Title                    = {Data {Visualization} for {Social} {Science}},

  Abstract                 = {A practical introduction with R and ggplot2.},
  File                     = {Snapshot:/home/bolker/.mozilla/firefox/f2nw6467.default/zotero/storage/MTXFEB3J/socviz.co.html:text/html},
  Url                      = {http://socviz.co/},
  Urldate                  = {2017-09-09}
}

@Misc{annkemery_data_2014,
  Title                    = {Data {Visualization} {Checklist}},
  Month                    = may,
  Year                     = {2014},

  Abstract                 = {Stephanie Evergreen and I created the Data Visualization Checklist to provide guidance on how, exactly, to make effective graphs},
  Journal                  = {Ann's Blog},
  Url                      = {http://annkemery.com/portfolio/dataviz-checklist/},
  Urldate                  = {2016-07-09}
}

@Article{noauthor_chance_2008,
  Title                    = {{CHANCE} {Graphic} {Display} {Contest}: {Burtin}'s {Antibiotic} {Data}},
  Journal                  = {CHANCE},
  Year                     = {2008},

  Month                    = sep,
  Number                   = {4},
  Pages                    = {62--62},
  Volume                   = {21},

  Doi                      = {10.1080/09332480.2008.10722935},
  ISSN                     = {0933-2480},
  Shorttitle               = {{CHANCE} {Graphic} {Display} {Contest}},
  Url                      = {https://doi.org/10.1080/09332480.2008.10722935},
  Urldate                  = {2018-01-15}
}



@article{leonhardt_rich_2019,
	chapter = {Opinion},
	title = {The {Rich} {Really} {Do} {Pay} {Lower} {Taxes} {Than} {You}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/interactive/2019/10/06/opinion/income-tax-rate-wealthy.html},
	abstract = {The 400 wealthiest Americans now pay a lower rate than the middle class.},
	language = {en-US},
	urldate = {2019-10-25},
	journal = {The New York Times},
	author = {Leonhardt, David},
	month = oct,
	year = {2019},
	keywords = {Corporate Taxes, Federal Taxes (US), High Net Worth Individuals, Income Inequality, Income Tax, Saez, Emmanuel, Tax Credits, Deductions and Exemptions, Tax Evasion, Taxation, United States Economy, United States Politics and Government, Zucman, Gabriel}
}


@article{koo_geovisualizing_2018,
	title = {Geovisualizing attribute uncertainty of interval and ratio variables: {A} framework and an implementation for vector data},
	volume = {44},
	shorttitle = {Geovisualizing attribute uncertainty of interval and ratio variables},
	journal = {Journal of Visual Languages \& Computing},
	author = {Koo, Hyeongmo and Chun, Yongwan and Griffith, Daniel A.},
	year = {2018},
	pages = {89--96}
}


@misc{hohle_cartograms_2016,
	title = {Cartograms with {R}},
	url = {http://staff.math.su.se/hoehle/blog/2016/10/10/cartograms.html},
	abstract = {We show how to create cartograms with R by illustrating the population and age-distribution of the planning regions of Berlin by static plots and animations.},
	urldate = {2019-10-27},
	journal = {Theory meets practice},
	author = {Höhle, Michael},
	month = oct,
	year = {2016}
}

@misc{perrier_topogram_2019,
	title = {{dreamRs}/topogram},
	url = {https://github.com/dreamRs/topogram},
	abstract = {R htmlwidget for cartogram-chart.},
	urldate = {2019-10-27},
	publisher = {dreamRs},
	author = {Perrier, Victor},
	month = oct,
	year = {2019},
	note = {original-date: 2016-10-10T20:29:36Z},
	keywords = {htmlwidgets, maps, r}
}


@article{maceachren_visualizing_2005,
	title = {Visualizing geospatial information uncertainty: {What} we know and what we need to know},
	volume = {32},
	shorttitle = {Visualizing geospatial information uncertainty},
	number = {3},
	journal = {Cartography and Geographic Information Science},
	author = {MacEachren, Alan M. and Robinson, Anthony and Hopper, Susan and Gardner, Steven and Murray, Robert and Gahegan, Mark and Hetzler, Elisabeth},
	year = {2005},
	pages = {139--160}
}


@book{bivand_applied_2013,
	address = {New York},
	edition = {2nd},
	title = {Applied {Spatial} {Data} {Analysis} with {R}},
	isbn = {978-1-4614-7617-7},
	abstract = {This book first presents R packages, functions, classes and methods for handling spatial data. It then showcases more specialised kinds of spatial data analysis, including spatial point pattern analysis, interpolation and geostatistics and disease mapping.},
	language = {English},
	publisher = {Springer},
	author = {Bivand, Roger S. and Pebesma, Edzer and Gómez-Rubio, Virgilio},
	month = jun,
	year = {2013}
}


@article{buja_interactive_1996,
	title = {Interactive {High}-{Dimensional} {Data} {Visualization}},
	volume = {5},
	issn = {1061-8600},
	url = {http://www.jstor.org/stable/1390754},
	doi = {10.2307/1390754},
	abstract = {We propose a rudimentary taxonomy of interactive data visualization based on a triad of data analytic tasks: finding Gestalt, posing queries, and making comparisons. These tasks are supported by three classes of interactive view manipulations: focusing, linking, and arranging views. This discussion extends earlier work on the principles of focusing and linking and sets them on a firmer base. Next, we give a high-level introduction to a particular system for multivariate data visualization--XGobi. This introduction is not comprehensive but emphasizes XGobi tools that are examples of focusing, linking, and arranging views; namely, high-dimensional projections, linked scatterplot brushing, and matrices of conditional plots. Finally, in a series of case studies in data visualization, we show the powers and limitations of particular focusing, linking, and arranging tools. The discussion is dominated by high-dimensional projections that form an extremely well-developed part of XGobi. Of particular interest are the illustration of asymptotic normality of high-dimensional projections (a theorem of Diaconis and Freedman), the use of high-dimensional cubes for visualizing factorial experiments, and a method for interactively generating matrices of conditional plots with high-dimensional projections. Although there is a unifying theme to this article, each section--in particular the case studies--can be read separately.},
	number = {1},
	urldate = {2018-01-01},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Buja, Andreas and Cook, Dianne and Swayne, Deborah F.},
	year = {1996},
	pages = {78--99}
}


@book{holmes_modern_2019,
	address = {Cambridge},
	edition = {1 edition},
	title = {Modern {Statistics} for {Modern} {Biology}},
	isbn = {978-1-108-70529-5},
	abstract = {If you are a biologist and want to get the best out of the powerful methods of modern computational statistics, this is your book. You can visualize and analyze your own data, apply unsupervised and supervised learning, integrate datasets, apply hypothesis testing, and make publication-quality figures using the power of R/Bioconductor and ggplot2. This book will teach you 'cooking from scratch', from raw data to beautiful illuminating output, as you learn to write your own scripts in the R language and to use advanced statistics packages from CRAN and Bioconductor. It covers a broad range of basic and advanced topics important in the analysis of high-throughput biological data, including principal component analysis and multidimensional scaling, clustering, multiple testing, unsupervised and supervised learning, resampling, the pitfalls of experimental design, and power simulations using Monte Carlo, and it even reaches networks, trees, spatial statistics, image data, and microbial ecology. Using a minimum of mathematical notation, it builds understanding from well-chosen examples, simulation, visualization, and above all hands-on interaction with data and code.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Holmes, Susan and Huber, Wolfgang},
	month = mar,
	year = {2019}
}


@article{mason_why_2019,
	title = {Why scientists need to be better at data visualization},
	url = {https://www.knowablemagazine.org/article/mind/2019/science-data-visualization},
	doi = {10.1146/knowable-110919-1},
	abstract = {The scientific literature is riddled with bad charts and graphs, leading to misunderstanding and worse. Avoiding design missteps can improve understanding of research.},
	urldate = {2019-11-18},
	journal = {Knowable Magazine},
	author = {Mason, Betsy},
	month = nov,
	year = {2019}
}



@article{zeileis_colorspace_2019,
	title = {colorspace: {A} {Toolbox} for {Manipulating} and {Assessing} {Colors} and {Palettes}},
	shorttitle = {colorspace},
	url = {http://arxiv.org/abs/1903.06490},
	abstract = {The R package colorspace provides a flexible toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in statistical graphics and data visualizations. In particular, the package provides a broad range of color palettes based on the HCL (Hue-Chroma-Luminance) color space. The three HCL dimensions have been shown to match those of the human visual system very well, thus facilitating intuitive selection of color palettes through trajectories in this space. Using the HCL color model general strategies for three types of palettes are implemented: (1) Qualitative for coding categorical information, i.e., where no particular ordering of categories is available. (2) Sequential for coding ordered/numeric information, i.e., going from high to low (or vice versa). (3) Diverging for coding ordered/numeric information around a central neutral value, i.e., where colors diverge from neutral to two extremes. To aid selection and application of these palettes the package also contains scales for use with ggplot2, shiny (and tcltk) apps for interactive exploration, visualizations of palette properties, accompanying manipulation utilities (like desaturation and lighten/darken), and emulation of color vision deficiencies.},
	urldate = {2019-11-18},
	journal = {arXiv:1903.06490},
	author = {Zeileis, Achim and Fisher, Jason C. and Hornik, Kurt and Ihaka, Ross and McWhite, Claire D. and Murrell, Paul and Stauffer, Reto and Wilke, Claus O.},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.06490 [cs, stat]},
	keywords = {Computer Science - Graphics, Statistics - Computation}
}


@article{cook_foundation_2021,
	title = {The {Foundation} is {Available} for {Thinking} about {Data} {Visualization} {Inferentially}},
	issn = {,},
	url = {https://hdsr.mitpress.mit.edu/pub/mpdasaqt/release/1},
	doi = {10.1162/99608f92.8453435d},
	language = {en},
	urldate = {2021-07-30},
	journal = {Harvard Data Science Review},
	author = {Cook, Dianne and Reid, Nancy and Tanaka, Emi},
	month = jul,
	year = {2021},
	note = {Publisher: PubPub}
}


@article{hullman_design_2021,
	title = {To design interfaces for exploratory data analysis, we need theories of graphical inference},
	url = {http://arxiv.org/abs/2104.02015},
	abstract = {Research and development in computer science and statistics have produced increasingly sophisticated software interfaces for interactive and exploratory analysis, optimized for easy pattern finding and data exposure. But design philosophies that emphasize exploration over other phases of analysis risk confusing a need for flexibility with a conclusion that exploratory visual analysis is inherently model-free and cannot be formalized. We describe how without a grounding in theories of human statistical inference, research in exploratory visual analysis can lead to contradictory interface objectives and representations of uncertainty that can discourage users from drawing valid inferences. We discuss how the concept of a model check in a Bayesian statistical framework unites exploratory and confirmatory analysis, and how this understanding relates to other proposed theories of graphical inference. Viewing interactive analysis as driven by model checks suggests new directions for software and empirical research around exploratory and visual analysis. For example, systems should enable specifying and explicitly comparing data to null and other reference distributions and better representations of uncertainty. Implications of Bayesian and other theories of graphical inference should be tested against outcomes of interactive analysis by people to drive theory development.},
	urldate = {2021-07-30},
	journal = {arXiv:2104.02015 [cs]},
	author = {Hullman, Jessica and Gelman, Andrew},
	month = jul,
	year = {2021},
	note = {arXiv: 2104.02015},
	keywords = {Computer Science - Human-Computer Interaction, H.5.2}
}

@misc{bryan_project-oriented_2017,
	title = {Project-oriented workflow},
	url = {https://www.tidyverse.org/blog/2017/12/workflow-vs-script/},
	abstract = {Advice on workflows for developing R scripts. How to think about whether an action belongs in the script or elsewhere.},
	language = {en-us},
	urldate = {2021-01-14},
	journal = {Tidyverse},
	author = {Bryan, Jenny},
	month = dec,
	year = {2017}
}


@inproceedings{matejka_datasaurus_2017,
	title = {The {Datasaurus} Dozen - Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing},
	url = {https://www.autodeskresearch.com/publications/samestats},
	doi = {10.1145/3025453.3025912},
	urldate = {2018-11-14},
	author = {Matejka, Justin and Fitzmaurice, George},
	year = {2017},
      booktitle = {ACM SIGCHI Conference on Human Factors in Computing Systems}
}


@article{sailynoja_graphical_2021,
	title = {Graphical {Test} for {Discrete} {Uniformity} and its {Applications} in {Goodness} of {Fit} {Evaluation} and {Multiple} {Sample} {Comparison}},
	url = {http://arxiv.org/abs/2103.10522},
	abstract = {Assessing goodness of fit to a given distribution plays an important role in computational statistics. The Probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation and optimization based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is especially useful in Markov chain Monte Carlo convergence diagnostics. We provide numerical experiments to assess the properties of the tests using both simulated and real world data and give recommendations on their practical application in computational statistics workflows.},
	urldate = {2021-04-07},
	journal = {arXiv:2103.10522 [stat]},
	author = {Säilynoja, Teemu and Bürkner, Paul-Christian and Vehtari, Aki},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.10522},
	keywords = {Statistics - Methodology}
}



@article{talts_validating_2020,
	title = {Validating {Bayesian} {Inference} {Algorithms} with {Simulation}-{Based} {Calibration}},
	url = {http://arxiv.org/abs/1804.06788},
	abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce {\textbackslash}emph\{simulation-based calibration\} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
	urldate = {2021-10-01},
	journal = {arXiv:1804.06788 [stat]},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv: 1804.06788},
	keywords = {Statistics - Methodology},
	annote = {Comment: 19 pages, 13 figures}
}


@article{cairo_ethical_2014,
	title = {Ethical infographics},
	volume = {37},
	url = {https://www.dropbox.com/s/pqgmg02yz0pgju4/EthicalInfographics.pdf},
	number = {2},
	journal = {IRE Journal},
	author = {Cairo, Alberto},
	year = {2014},
	pages = {25--27}
}

@article{cairo_if_2020,
	title = {If {Anything} on {This} {Graphic} {Causes} {Confusion}, {Discard} the {Entire} {Product}},
	volume = {40},
	issn = {1558-1756},
	doi = {10.1109/MCG.2019.2961716},
	abstract = {Reading a visualization is often an experience mediated by people other than designers themselves. In more than two decades designing and teaching visualization, I have learned to embrace the mantra "what you design isn't what people see." It applies not only to visualizations, but to most artifacts, tools, and technologies.},
	number = {2},
	journal = {IEEE Computer Graphics and Applications},
	author = {Cairo, Alberto},
	month = mar,
	year = {2020},
	note = {Conference Name: IEEE Computer Graphics and Applications},
	keywords = {Cognitive science, Data visualization, Hurricanes, Uncertainty, Visualization},
	pages = {91--97}
}




@article{bolin_quantifying_2017,
	title = {Quantifying the {Uncertainty} of {Contour} {Maps}},
	volume = {26},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2016.1228537},
	doi = {10.1080/10618600.2016.1228537},
	abstract = {Contour maps are widely used to display estimates of spatial fields. Instead of showing the estimated field, a contour map only shows a fixed number of contour lines for different levels. However, despite the ubiquitous use of these maps, the uncertainty associated with them has been given a surprisingly small amount of attention. We derive measures of the statistical uncertainty, or quality, of contour maps, and use these to decide an appropriate number of contour lines, which relates to the uncertainty in the estimated spatial field. For practical use in geostatistics and medical imaging, computational methods are constructed, that can be applied to Gaussian Markov random fields, and in particular be used in combination with integrated nested Laplace approximations for latent Gaussian models. The methods are demonstrated on simulated data and an application to temperature estimation is presented.},
	number = {3},
	urldate = {2021-10-21},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Bolin, David and Lindgren, Finn},
	month = jul,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10618600.2016.1228537},
	keywords = {Excursions, Isolines, Kriging, Latent Gaussian processes, Spatial uncertainty visualization},
	pages = {513--524},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/6ZGG3R2W/10618600.2016.html:text/html;Submitted Version:/home/bolker/Documents/zotero_new/storage/V956KGP7/Bolin and Lindgren - 2017 - Quantifying the Uncertainty of Contour Maps.pdf:application/pdf},
}

@inproceedings{correll_value-suppressing_2018,
	address = {Montreal QC Canada},
	title = {Value-{Suppressing} {Uncertainty} {Palettes}},
	isbn = {978-1-4503-5620-6},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174216},
	doi = {10.1145/3173574.3174216},
	abstract = {Understanding uncertainty is critical for many analytical tasks. One common approach is to encode data values and uncertainty values independently, using two visual variables. These resulting bivariate maps can be difﬁcult to interpret, and interference between visual channels can reduce the discriminability of marks. To address this issue, we contribute ValueSuppressing Uncertainty Palettes (VSUPs). VSUPs allocate larger ranges of a visual channel to data when uncertainty is low, and smaller ranges when uncertainty is high. This non-uniform budgeting of the visual channels makes more economical use of the limited visual encoding space when uncertainty is low, and encourages more cautious decisionmaking when uncertainty is high. We demonstrate several examples of VSUPs, and present a crowdsourced evaluation showing that, compared to traditional bivariate maps, VSUPs encourage people to more heavily weight uncertainty information in decision-making tasks.},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Correll, Michael and Moritz, Dominik and Heer, Jeffrey},
	month = apr,
	year = {2018},
	pages = {1--11},
	file = {Correll et al. - 2018 - Value-Suppressing Uncertainty Palettes.pdf:/home/bolker/Documents/zotero_new/storage/7P4Y7L7M/Correll et al. - 2018 - Value-Suppressing Uncertainty Palettes.pdf:application/pdf},
}

@incollection{schermann_ethics_2019,
	title = {Ethics (chapter 5)},
	date = {2019},
	url = {https://mschermann.github.io/data_viz_reader/ethics.html},
	abstract = {This is the class reader for MSIS 2629.},
	urldate = {2021-11-01},
	booktitle = {A {Reader} on {Data} {Visualization}},
	author = {Schermann, Michael},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/Z3MW8F4B/ethics.html:text/html},
}

@misc{skau_code_2012,
	title = {A {Code} of {Ethics} for {Data} {Visualization} {Professionals}},
	url = {https://rockcontent.com/blog/a-code-of-ethics-for-data-visualization-professionals/},
	abstract = {A Code of Ethics for Data Visualization Professionals. Insights to help you grow your business.},
	language = {en-US},
	urldate = {2021-11-01},
	journal = {Rock Content},
	author = {Skau, Drew},
	month = feb,
	year = {2012},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/KUPWFCNR/a-code-of-ethics-for-data-visualization-professionals.html:text/html},
}


@article{lundgard_accessible_2022,
	title = {Accessible {Visualization} via {Natural} {Language} {Descriptions}: {A} {Four}-{Level} {Model} of {Semantic} {Content}},
	shorttitle = {Accessible {Visualization} via {Natural} {Language} {Descriptions}},
	url = {http://vis.csail.mit.edu/pubs/vis-text-model/},
	doi = {10.1109/TVCG.2021.3114770},
	abstract = {Visualizations like “Flatten the Curve” (A) efficiently communicate critical public health information, while simultaneously excluding people with disabilities [11, 28]. To promote accessible visualization via natural language descriptions (B, C), we introduce a four-level model of semantic content. Our model categorizes and color codes sentences according to the semantic content they convey. Abstract Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization. 1 Introduction The proliferation of visualizations during the COVID-19 pandemic has underscored their double-edged potential: efficiently communicating critical public health information — as with the immediately-canonical “Flatten the Curve” chart (Fig. Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content) — while simultaneously excluding people with disabilities. {\textbackslash}sayFor many people with various types of disabilities, graphics and the information conveyed in them is hard to read and understand, says software engineer Tyler Littlefield [28], who built a popular text-based COVID-19 statistics tracker after being deluged with inaccessible infographics [94, 65]. While natural language descriptions sometimes accompany visualizations in the form of chart captions or alt text (short for {\textbackslash}sayalternative text), these practices remain rare. Technology educator and researcher Chancey Fleet notes that infographics and charts usually lack meaningful and detailed descriptions, leaving disabled people with {\textbackslash}saya feeling of uncertainty about the pandemic [28]. For readers with visual disabilities (approximately 8.1 million in the United States and 253 million worldwide [1]), inaccessible visualizations are, at best, demeaning and, at worst, damaging to health, if not accompanied by meaningful and up-to-date alternatives. Predating the pandemic, publishers and education specialists have long suggested best practices for accessible visual media, including guidelines for tactile graphics [41] and for describing “complex images” in natural language [99, 39]. While valuable, visualization authors have yet to broadly adopt these practices, for lack of experience with accessible media, if not a lack of attention and resources. Contemporary visualization research has primarily attended to color vision deficiency [21, 77, 79], and has only recently begun to engage with non-visual alternatives [25, 67] and with accessibility broadly [53, 105]. Parallel to these efforts, computer science researchers have been grappling with the engineering problem of automatically generating chart captions [27, 78, 84]. While well-intentioned, these methods usually neither consult existing accessibility guidelines, nor do they evaluate their results empirically with their intended readership. As a result, it is difficult to know how useful (or not) the resultant captions are, or how effectively they improve access to meaningful information. In this paper, we make a two-fold contribution. First, we extend existing accessibility guidelines by introducing a conceptual model for categorizing and comparing the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 natural language sentences, authored by over 120 participants in an online study (§ 3), our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context) (§ 4). Second, we demonstrate how this model can be applied to evaluate the effectiveness of visualization descriptions, by comparing different semantic content levels and reader groups. We conduct a mixed-methods evaluation in which a group of 30 blind and 90 sighted readers rank the usefulness of descriptions authored at varying content levels (§ 5). Analyzing the resultant 3,600 ranked descriptions, we find significant differences in the content favored by these reader groups: while both groups generally prefer mid-level semantic content, they sharply diverge in their rankings of both the lowest and highest levels of our model. These findings, contextualized by readers’ open-ended feedback, suggest that access to meaningful information is strongly reader-specific, and that captions for blind readers should aim to convey a chart’s trends and statistics, rather than solely detailing its low-level design elements or high-level insights. Our model of semantic content is not only descriptive (categorizing what is conveyed by visualizations) and evaluative (helping us to study what should be conveyed to whom) but also generative [7, 8], pointing toward novel multimodal and accessible data representations (§ 6.1). Our work further opens a space of research on natural language as a data interface coequal with the language of graphics [12], calling back to the original linguistic and semiotic motivations at the heart of visualization theory and design (§ 6.2). 2 Related Work Multiple visualization-adjacent literatures have studied methods for describing charts and graphics through natural language — including accessible media research, Human-Computer Interaction (HCI), Computer Vision (CV), and Natural Language Processing (NLP). But, these various efforts have been largely siloed from one another, adopting divergent methods and terminologies (e.g., the terms “caption” and “description” are used inconsistently). Here, we survey the diverse terrain of literatures intersecting visualization and natural language. 2.1 Automatic Methods for Visualization Captioning Automatic methods for generating visualization captions broadly fall into two categories: those using CV and NLP methods when the chart is a rasterized image (e.g., jpegs or pngs); and those using structured specifications of the chart’s construction (e.g., grammars of graphics). 2.1.1 Computer Vision and Natural Language Processing Analogous to the long-standing CV and NLP problem of automatically captioning photographic images [64, 58, 48], recent work on visualization captioning has aimed to automatically generate accurate and descriptive natural language sentences for charts [22, 24, 23, 6, 78, 59, 83]. Following the encoder-decoder framework of statistical machine translation [98, 107], these approaches usually take rasterized images of visualizations as input to a CV model (the encoder), which learns the visually salient features for outputting a relevant caption via a language model (the decoder). Training data consists of ⟨chart, caption⟩  pairs, collected via web-scraping and crowdsourcing [84], or created synthetically from pre-defined sentence templates [47]. While these approaches are well-intentioned, in aiming to address the engineering problem of how to automatically generate natural language captions for charts, they have largely sidestepped the complementary (and prior) question: which semantic content should be generated to begin with? Some captions may be more or less descriptive than others, and different readers may receive different semantic content as more or less useful, depending on their levels of data literacy, domain-expertise, and/or visual perceptual ability [69, 72, 71]. To help orient work on automatic visualization captioning, our four-level model of semantic content offers a means of asking and answering these more human-centric questions. 2.1.2 Structured Visualization Specifications In contrast to rasterized images of visualizations, chart templates [96], component-based architectures [38], and grammars of graphics [87] provide not only a structured representation of the visualization’s construction, but typically render the visualization in a structured manner as well. For instance, most of these approaches either render the output visualization as Scalable Vector Graphics (SVG) or provide a scenegraph API. Unfortunately, these output representations lose many of the semantics of the structured input (e.g., which elements correspond to axes and legends, or how nesting corresponds to visual perception). As a result, most present-day visualizations are inaccessible to people who navigate the web using screen readers. For example, using Apple’s VoiceOver to read D3 charts rendered as SVG usually outputs an inscrutable mess of screen coordinates and shape rendering properties. Visualization toolkits can ameliorate this by leveraging their structured input to automatically add Accessible Rich Internet Application (ARIA) attributes to appropriate output elements, in compliance with the World Wide Web Consortium (W3C)’s Web Accessibility Initiative (WAI) guidelines [99]. Moreover, this structured input representation can also simplify automatically generating natural language captions through template-based mechanisms, as we discuss in § 4.1. 2.2 Accessible Media and Human-Computer Interaction While automatic methods researchers often note accessibility as a worthy motivation [27, 78, 84, 83, 30, 31], evidently few have collaborated directly with disabled people [25, 71] or consulted existing accessibility guidelines [67]. Doing so is more common to HCI and accessible media literatures [73, 91], which broadly separate into two categories corresponding to the relative expertise of the description authors: those authored by experts (e.g., publishers of accessible media) and those authored by non-experts (e.g., via crowdsourcing or online platforms). 2.2.1 Descriptions Authored by Experts Publishers have developed guidelines for describing graphics appearing in science, technology, engineering, and math (STEM) materials [39, 9]. Developed by and for authors with some expert accessibility knowledge, these guidelines provide best practices for conveying visualized content in traditional media (e.g., printed textbooks, audio books, and tactile graphics). But, many of their prescriptions — particularly those relating to the content conveyed by a chart, rather than the modality through which the chart is rendered — are also applicable to web-based visualizations. Additionally, web accessibility guidelines from W3C provide best-practices for writing descriptions of “complex images” (including canonical chart types), either in a short description alt text attribute, or as a long textual description displayed alongside the visual image [99]. While some of these guidelines have been adopted by visualization practitioners [19, 88, 29, 32, 102, 101, 34], we here bring special attention to the empirically-grounded and well-documented guidelines created by the wgbh National Center for Accessible Media [39] and by the Benetech Diagram Center [9]. 2.2.2 Descriptions Authored by Non-Experts Frequently employed in HCI and visualization research, crowdsourcing is a technique whereby remote non-experts complete tasks currently infeasible for automatic methods, with applications to online accessibility [13], as well as remote description services like Be My Eyes. For example, Morash et al. explored the efficacy of two types of non-expert tasks for authoring descriptions of visualizations: non-experts authoring free-form descriptions without expert guidance, versus those filling-in sentence templates pre-authored by experts [72]. While these approaches can yield more richly detailed and “natural”-sounding descriptions (as we discuss in § 5), and also provide training data for auto-generated captions and annotations [84, 56], it is important to be attentive to potential biases in human-authored descriptions [10]. 2.3 Natural Language Hierarchies and Interfaces Apart from the above methods for generating descriptions, prior work has adopted linguistics-inspired framings to elucidate how natural language is used to describe — as well as interact with — visualizations. 2.3.1 Using Natural Language to Describe Visualizations Demir et al. have proposed a hierarchy of six syntactic complexity levels corresponding to a set of propositions that might be conveyed by bar charts [27]. Our model differs in that it orders semantic content — i.e., what meaning the natural language sentence conveys — rather than how it does so syntactically. Thus, our model is agnostic to a sentence’s length, whether it contains multiple clauses or conjunctions, which has also been a focus of prior work in automatic captioning [84]. Moreover, whereas Demir et al. speculatively “envision” their set of propositions to construct their hierarchy, we arrive to our model empirically through a multi-stage grounded theory process (§ 3). Perhaps closest to our contribution are a pair of papers by Kosslyn [57] and Livingston \& Brock [66]. Kosslyn draws on canonical linguistic theory, to introduce three levels for analyzing charts: the syntactic relationship between a visualization elements; the semantic meaning of these elements in what they depict or convey; and the pragmatic aspects of what these elements convey in the broader context of their reading [57]. We seeded our model construction with a similar linguistics-inspired framing, but also evaluated it empirically, to further decompose the semantic levels (§ 3.1). Livingston \& Brock adapt Kosslyn’s ideas to generate what they call “visual sentences”: natural language sentences that are the result of executing a single, specific analytic task against a visualization [66]. Inspired by the Sentence Verification Technique (svt) [85, 86], this work considers visual sentences for assessing graph comprehension, hoping to offer a more “objective” and automated alternative to existing visualization literacy assessments [63, 35]. While we adopt a more qualitative process for constructing our model, Livingston \& Brock’s approach suggests opportunities for future work: might our model map to similarly-hierarchical models of analytic tasks [17, 5]? 2.3.2 Using Natural Language to Interact with Visualizations Adjacently, there is a breadth of work on Natural Language Interfaces (NLIs) for constructing and exploring visualizations [75, 42, 90, 50]. While our model primarily considers the natural language sentences that are conveyed by visualizations (cf., natural language as input for chart specification and exploration) [93], our work may yet have implications for NLIs. For example, Hearst et al. have found that many users of chatbots prefer not to see charts and graphics alongside text in the conversational dialogue interface [43]. By helping to decouple visual-versus-linguistic data representations, our model might be applied to offer these users a textual alternative to inline charts. Thus, we view our work as complementary to NLIs, facilitating multimodal and more accessible data representations [51], while helping to clarify the theoretical relationship between charts and captions [52, 80], and other accompanying text [106, 54, 55, 2]. 3 Constructing the Model: Employing the Grounded Theory Methodology To construct our model of semantic content we conducted a multi-stage process, following the grounded theory methodology. Often employed in HCI and the social sciences, grounded theory offers a rigorous method for making sense of a domain that lacks a dominant theory, and for constructing a new theory that accounts for diverse phenomena within that domain [74]. The methodology approaches theory construction inductively — through multiple stages of inquiry, data collection, “coding” (i.e., labeling and categorizing), and refinement — as well as empirically, remaining strongly based (i.e., “grounded”) in the data [74]. To construct our model of semantic content, we proceeded in two stages. First, we conducted small-scale data collection and initial open coding to establish preliminary categories of semantic content. Second, we gathered a larger-scale corpus to iteratively refine those categories, and to verify their coverage over the space of natural language descriptions. 3.1 Initial Open Coding We began gathering preliminary data by searching for descriptions accompanying visualizations in journalistic publications (including the websites of FiveThirtyEight, the New York Times and the Financial Times), but found that these professional sites usually provided no textual descriptions — neither as a caption alongside the chart, nor as alt text for screen readers. Indeed, often these sites were engineered so that screen readers would pass over the visualizations entirely, as if they did not appear on the page at all. Thus, to proceed with the grounded theory method, we conducted initial open coding (i.e., making initial, qualitative observations about our data, in an “open-minded” fashion) by studying preliminary data from two sources. We collected 330 natural language descriptions from over 100 students enrolled in a graduate-level data visualization class. As a survey-design pilot to inform future rounds of data collection (§ 3.2.1), these initial descriptions were collected with minimal prompting: students were instructed to simply “describe the visualization” without specifying what kinds of semantic content that might include. The described visualizations covered a variety of chart types (e.g., bar charts, line charts, scatter plots) as well as dataset domains (e.g., public health, climate change, and gender equality). To complement the student-authored descriptions, from this same set of visualizations, we curated a set of 20 and wrote our (the authors’) own descriptions, attempting to be as richly descriptive as possible. Throughout, we adhered to a linguistics-inspired framing by attending to the semantic and pragmatic aspects of our writing: which content could be conveyed through the graphical sign-system alone, and which required drawing upon our individual background knowledge, experiences, and contexts. Analyzing these preliminary data, we proceeded to the next stage in the grounded theory method: forming axial codes (i.e., open codes organized into broader abstractions, with more generalized meaning [74]) corresponding to different content. We began to distinguish between content about a visualization’s construction (e.g., its title, encodings, legends), content about trends appearing in the visualized data (e.g., correlations, clusters, extrema), and content relevant to the visualized data but not represented in the visualization itself (e.g., explanations based on current events and domain-specific knowledge). From these axial codes, different categories (i.e., groupings delineated by shared characteristics of the content) began to emerge [74], corresponding to a chart’s encoded elements, latent statistical relations, perceptual trends, and context. We refined these content categories iteratively by first writing down descriptions of new visualizations (again, as richly as possible), and then attempting to categorize each sentence appearing in that description. If we encountered a sentence that didn’t fit within any category, we either refined the specific characteristics belonging to an existing category, or we created a new category, where appropriate. Table 1: Breakdown of the 50 curated visualizations, across the three dimensions: type, topic, and difficulty. (N.b., each column sums to 50.) chart type topic difficulty bar 18 academic 15 easy 21 line 21 business 18 medium 20 scatter 11 journalism 17 hard 9 3.2 Gathering A Corpus The prior inductive and empirical process resulted in a set of preliminary content categories. To test their robustness, and to further refine them, we conducted an online survey to gather a larger-scale corpus of 582 visualization descriptions comprised of 2,147 sentences. 3.2.1 Survey Design We first curated a set of 50 visualizations drawn from the MassVis dataset [16, 15], Quartz’s Atlas visualization platform [81], examples from the Vega-Lite gallery [87], and the aforementioned journalistic publications. We organized these visualizations along three dimensions: the visualization type (bar charts, line charts, and scatter plots); the topic of the dataset domain (academic studies, business-related, or non-business data journalism); and their difficulty based on an assessment of their visual and conceptual complexity. We labeled visualizations as “easy” if they were basic instances of their canonical type (e.g., single-line or un-grouped bar charts), as ”medium” if they were more moderate variations on canon (e.g., contained bar groupings, overlapping scatterplot clusters, visual embellishments, or simple transforms), and as ”hard” if they strongly diverged from canon (e.g., contained chartjunk or complex transforms such as log scales). To ensure robustness, two authors labeled the visualizations independently, and then resolved any disagreement through discussion. Table 1 summarizes the breakdown of the 50 visualizations across these three dimensions. In the survey interface, participants were shown a single, randomly-selected visualization at a time, and prompted to describe it in complete English sentences. In our preliminary data collection (§ 3.1), we found that without explicit prompting participants were likely to provide only brief and minimally informative descriptions (e.g., sometimes simply repeating the chart title and axis labels). Thus, to mitigate against this outcome, and to elicit richer semantic content, we explicitly instructed participants to author descriptions that did not only refer to the chart’s basic elements and encodings (e.g., it’s title, axes, colors) but to also referred to other content, trends, and insights that might be conveyed by the visualization. To make these instructions intelligible, we provided participants with a few pre-generated sentences enumerating the visualization’s basic elements and encodings (e.g., the color coded sentences in Table 3 A.1, B.1, C.1), and prompted them to author semantic content apart from what was already conveyed by those sentences. To avoid biasing their responses, participants were not told that they would be read by people with visual disabilities. This prompting ensured that the survey captured a breadth of semantic content, and not only the most readily-apparent aspects of the visualization’s construction. Figure 1: A visual “fingerprint” [49] of our corpus, faceted by chart type and difficulty. Each row corresponds to a single chart. Each column shows a participant-authored description for that chart, color coded according to our model. The first column shows the provided Level 1 prompt. 3.2.2 Survey Results We recruited 120 survey participants through the Prolific platform. In an approximately 30-minute study compensated at a rate of \$10-12 per hour, we asked each participant to describe 5 visualizations (randomly selected from the set of 50), resulting in at least 10 participant-authored descriptions per visualization. For some visualizations, we collected between 10-15 responses, due to limitations of the survey logic for randomly selecting a visualization to show participants. In total, this survey resulted in 582 individual descriptions comprised of 2,147 natural language sentences. We manually cleaned each sentence to correct errors in spelling, grammar, punctuation (n.b., we did not alter the semantic content conveyed by each sentence). We then labeled each sentence according to the content categories developed through our prior grounded theory process. As before, to ensure robustness, two authors labeled each sentence independently, and then resolved any disagreement through discussion. This deliberative and iterative process helped us to further distinguish and refine our categories. For example, we were able to more precisely draw comparisons between sentences reporting computable “data facts” [92, 100] through rigid or templatized articulation (such as {\textbackslash}say[x-encoding] is positively correlated with [y-encoding]), with sentences conveying the same semantic content through more “natural”-sounding articulation (such as {\textbackslash}sayfor the most part, as [x-encoding] increases, so too does [y-encoding]). In summary, the entire grounded theory process resulted in four distinct semantic content categories, which we organize into levels in the next section. A visual “fingerprint” [49] shows how semantic content is distributed across sentences in the corpus (Fig. 1). Level 1 (consisting of a chart’s basic elements and encodings) represents 9.1\% of the sentences in the corpus. This is expected, since Level 1 sentences were pre-generated and provided as a prompt to our survey participants, as we previously discussed. The distribution of sentences across the remaining levels is as follows: Level 2 (35.1\%), Level 3 (42.9\%), and Level 4 (12.9\%). The fairly-balanced distribution suggests that our survey prompting successfully captured natural language sentences corresponding to a breadth of visualized content. 4 A Four-Level Model of Semantic Content Table 2: A four-level model of semantic content for accessible visualization. Levels are defined by the semantic content conveyed by natural language descriptions of visualizations. Additionally, we offer computational considerations for generating the semantic content at each level of the model. \# level keywords semantic content computational considerations {\textbackslash}cellcolorLevel44 {\textbackslash}cellcolorLevel4Lightcontextual and domain-specific {\textbackslash}cellcolorLevel4Lightdomain-specific insights, current events, social and political context, explanations {\textbackslash}cellcolorLevel4Lightcontextual knowledge and domain-specific expertise (perceiver-dependent) {\textbackslash}cellcolorLevel33 {\textbackslash}cellcolorLevel3Lightperceptual and cognitive {\textbackslash}cellcolorLevel3Lightcomplex trends, pattern synthesis, exceptions, commonplace concepts {\textbackslash}cellcolorLevel3Lightreference to the rendered visualization and “common knowledge” (perceiver-dependent) {\textbackslash}cellcolorLevel22 {\textbackslash}cellcolorLevel2Lightstatistical and relational {\textbackslash}cellcolorLevel2Lightdescriptive statistics, extrema, outliers, correlations, point-wise comparisons {\textbackslash}cellcolorLevel2Lightaccess to the visualization specification or backing dataset (perceiver-independent) {\textbackslash}cellcolorLevel11 {\textbackslash}cellcolorLevel1Lightelemental and encoded {\textbackslash}cellcolorLevel1Lightchart type, encoding channels, title, axis ranges, labels, colors {\textbackslash}cellcolorLevel1Lightaccess to the visualization specification or rasterized image (perceiver-independent) Our grounded theory process yielded a four-level model of semantic content for the natural language description of visualizations. In the following subsections, we introduce the levels of the model and provide example sentences for each. Table 2 summarizes the levels, and Table 3 shows example visualizations from our corpus and corresponding descriptions, color coded according to the model’s color scale. Additionally, we offer practical computational considerations regarding the feasibility of generating sentences at each level, with reference to the present-day state-of-the-art methods described in Related Work. While we present them alongside each other for ease of explication, we emphasize that the model levels and computational considerations are theoretically decoupled: the model is indexed to the semantic content conveyed by natural language sentences, not to the computational means through which those sentences may or may not be generated. 4.1 Level 1: Elemental and Encoded Properties At the first level, there are sentences whose semantic content refers to elemental and encoded properties of the visualization (i.e., the visual components that comprise a graphical representation’s design and construction). These include the chart type (bar chart, line graph, scatter plot, etc.), its title and legend, its encoding channels, axis labels, and the axis scales. Consider the following sentence (Table 3.A.1). Mortality rate is plotted on the vertical y-axis from 0 to 15\%. Age is plotted on the horizontal x-axis in bins: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80+. This sentence “reads off” the axis labels and scales as they appear in the bar chart, with no additional synthesizing or interpretation. Sentences such as this are placed at the lowest level in the model because they refer to content that is foundational to visualization construction—comprising the elemental properties of the “language” of graphics [12]. Computational Considerations. Semantic content at Level 1 is so foundational that it has long been formalized — not only theoretically, as in Bertin’s Semiology of Graphics, but also mathematically and programmatically, as a “grammar of graphics” that precisely defines the algorithmic rules for constructing canonical chart types. [104]. In the case of these construction grammars, Level 1 content is directly encoded in the visualization’s structured specification (i.e., mappings between data fields and visual properties) [87]. Thus, for these grammars, generating sentences at Level 1 can amount to “filling in the blank” for a pre-defined sentence template. For example, given an appropriate template, the following natural language sentence could be trivially computed using the data encoded in the visualization specification. {\textbackslash}sayThis is a [chart-type] entitled [chart-title]. [y-encoding] is plotted on the vertical y-axis from [y-min] to [y-max]. [x-encoding] is plotted on the horizontal x-axis from [x-min] to [x-max]. And similarly, for other sentence templates and elemental properties encoded in the visualization’s structured specification. If the structured specification is not available, however, or if it does not follow a declarative grammar, then CV and NLP methods have also shown promise when applied to rasterized visualization images (e.g., jpegs or pngs). For example, recent work has shown that Level 1 semantic content can be feasibly generated provided an appropriate training dataset of pre-defined sentence templates [47], or by extracting a visualization’s structured specification from a rasterized visualization image [81]. 4.2 Level 2: Statistical Concepts and Relations At the second level, there are sentences whose semantic content refers to abstract statistical concepts and relations that are latent the visualization’s backing dataset. This content conveys computable descriptive statistics (such as mean, standard deviation, extrema, correlations) — what have sometimes been referred to as “data facts” because they are “objectively” present within a given dataset [92, 100] (as opposed to primarily observed via visualization, which affords more opportunities for subjective interpretation). In addition to these statistics, Level 2 content includes relations between data points (such as “greater than” or “lesser than” comparisons). Consider the following sentences (Table 3.C.2). For low income countries, the average life expectancy is 60 years for men and 65 years for women. For high income countries, the average life expectancy is 77 years for men and 82 years for women. These two sentences refer to a statistical property: the computed mean of the life expectancy of a population, faceted by gender and country income-level. Consider another example (Table 3.A.2). The highest COVID-19 mortality rate is in the 80+ age range, while the lowest mortality rate is in 10-19, 20-29, 30-39, sharing the same rate. Although this sentence is more complex, it nevertheless resides at Level 2. It refers to the extrema of the dataset (i.e., the “highest” and “lowest” mortality rates), and makes two comparisons (i.e., a comparison between the extrema, and another between age ranges sharing the lowest mortality rate). All of the above sentences above share the same characteristic, distinguishing them from those at Level 1: they refer to relations between points in the dataset, be they descriptive statistics or point-wise comparisons. Whereas Level 1 sentences “read off” the visualization’s elemental properties, Level 2 sentences “report” statistical concepts and relations within the chart’s backing dataset. Computational Considerations. While semantic content at Level 1 requires only reference to the visualization’s specification, content at Level 2 also requires access to the backing dataset. Here, the two categories of automatic methods begin to diverge in their computational feasibility. For visualizations with a structured specification, generating sentences at Level 2 is effectively as easy as generating sentences at Level 1: it requires little more computation to calculate and report descriptive statistics when the software has access to the backing dataset (i.e., encoded as part of the visualization specification). Indeed, many visualization software systems (such as Tableau’s Summary Card, Voder [92], Quill nlg Plug-In for Power BI, and others) automatically compute summary statistics and present them in natural language captions. By contrast, for CV and NLP methods, generating Level 2 sentences from a rasterized image is considerably more difficult — although not entirely infeasible — depending on the chart type and complexity. For example, these methods can sometimes report extrema (e.g., which age ranges exhibit the highest and lowest mortality rates in 3.A.2) [78, 26]. Nevertheless, precisely reporting descriptive statistics (e.g., the computed mean of points in a scatter plot) is less tractable, without direct access to the chart’s backing dataset. Table 3: Example visualizations and descriptions from our corpus. Paragraph breaks in rows A and B indicate a description authored by a unique participant from our corpus gathering survey (§ 3.2.1), while row C shows an curated exemplar description from our evaluation (§ 5.1). visualization description {\textbackslash}cellcolorGrayLightA [bar, easy, journalism] [1] This is a vertical bar chart entitled “COVID-19 mortality rate by age” that plots Mortality rate by Age. Mortality rate is plotted on the vertical y-axis from 0 to 15\%. Age is plotted on the horizontal x-axis in bins: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80+. [2] The highest COVID-19 mortality rate is in the 80+ age range, while the lowest mortality rate is in 10-19, 20-29, 30-39, sharing the same rate. [3] COVID-19 mortality rate does not linearly correspond to the demographic age. [4] The mortality rate increases with age, especially around 40-49 years and upwards. [5] This relates to people’s decrease in their immunity and the increase of co-morbidity with age. [6] The mortality rate increases exponentially with older people. [7] There is no difference in the mortality rate in the range between the age of 10 and 39. [8] The range of ages between 60 and 80+ are more affected by COVID-19. [9] We can observe that the mortality rate is higher starting at 50 years old due to many complications prior. [10] As we decrease the age, we also decrease the values in mortality by a lot, almost to none. {\textbackslash}cellcolorGrayLightB [line, medium, business] [1] This is a multi-line chart entitled “Big Tech Stock Prices” that plots price by date. The corporations include AAPL (Apple), AMZN (Amazon), GOOG (Google), IBM (IBM), and MSFT (Microsoft). The years are plotted on the horizontal x-axis from 2000 to 2010 with an increment of 2 years. The prices are plotted on the vertical y-axis from 0 to 800 with an increment of 200. [2] GOOG has the greatest price over time. MSFT has the lowest price over time. [3] Prices of particular Big Tech corporations seem to fluctuate but nevertheless increase over time. Years 2008-2009 are exceptions as we can see an extreme drop in prices of all given corporations. [4] The big drop in prices was caused by financial crisis of 2007-2008. The crisis culminated with the bankruptcy of Lehman Brothers on September 15, 2008 and an international banking crisis. [5] At the beginning of 2008, every of this stock price went down, likely due to the financial crisis. [6] Then they have risen again and dropped again, more so than previously. [7] GOOG has the highest price over the years. MSFT has the lowest price over the years. [8] GOOG quickly became the richest one of the Big Tech corporations. [9] GOOG had experienced some kind of a crisis in 2009, because their prices drop rapidly, but then rebounded. {\textbackslash}cellcolorGrayLightC [scatter, hard, academic] [1] This is a scatter plot entitled “Born in 2016: Life Expectancy Gap by Gender and Income” that plots Women Life Expectancy at Birth (Years) by Men Life Expectancy at Birth (Years). The Women Life Expectancy at Birth is plotted on the vertical y-axis from 40 to 90 years. The Men Life Expectancy at Birth is plotted on the horizontal x-axis from 40 to 90 years. High Income Countries are plotted in dark green. Low Income Countries are plotted in light green. A 45 degree line from the origin represents Equal Life Expectancy. [2] For low income countries, the average life expectancy is 60 years for men and 65 years for women. For high income countries, the average life expectancy is 77 years for men and 82 years for women. [3] Overall, women have a slightly higher life expectancy than men. Women live around 5 to 10 years longer than men. The low income countries are more scattered than the high income countries. There is a visible gap between high and low income countries, indicated by the Income-Age Divide line. [4] People living in low-income countries tend to have a lower life expectancy than the people living in high-income countries, likely due to many societal factors, including access to healthcare, food, other resources, and overall quality of life. People who live in lower income countries are more likely to experience deprivation and poverty, which can cause related health problems. 4.3 Level 3: Perceptual and Cognitive Phenomena At the third level, there are sentences whose semantic content refers to perceptual and cognitive phenomena appearing in the visual representation of the data. When compared to, and defended against, other forms of data analysis (e.g., purely mathematical or statistical methods), visualization is often argued to confer some unique benefit to human readers. That is, visualizations do not only “report” descriptive statistics of the data (as in Level 2), they also show their readers something more: they surface unforeseen trends, convey complex multi-faceted patterns, and identify noteworthy exceptions that aren’t readily apparent through non-visual methods of analysis (cf., Anscombe’s Quartet or the Datasaurus Dozen [70]). Level 3 sentences are comprised of content that refers to these perceptual and cognitive phenomena, usually articulated in “natural”-sounding (rather than templatized) language. Consider the following examples (Table 3.B.3 and 3.C.3, respectively). Prices of particular Big Tech corporations seem to fluctuate but nevertheless increase over time. Years 2008-2009 are exceptions as we can see an extreme drop in prices of all given corporations. The low income countries are more scattered than the high income countries. There is a visible gap between high and low income countries, indicated by the Income-Age Divide line. These sentences convey the “overall gist” of complex trends and patterns (e.g., stock prices “seem to fluctuate but nevertheless increase”), synthesize multiple trends to identify exceptions (e.g., “years 2008-2009 are exceptions as we can see an extreme drop” of multiple graphed lines at that point in time), and do so in “natural”-sounding language, by referencing commonplace concepts (such as “fluctuate”, “extreme drop”, “visible gap”). N.b., “natural”-sounding articulation is necessary but insufficient for Level 3 membership, as it is also possible to articulate Level 1 or 2 content in a non-templatized fashion (§ 3.2.2). Computational Considerations. At Level 3, we begin to reach and exceed the limits of present-day state-of-the-art automatic methods. While there exist “off-the-shelf” statistical packages for computing basic trends and predictions in a dataset (e.g., correlations, polynomial regressions, statistical inferences), visualizations allow us to perceive and articulate complex trends for which there may exist no line of “best fit”. While automatic methods may eventually approach (or exceed) human capabilities on well-defined tasks [78], for now Level 3 semantic content is likely generated via human (rather than machine) perception and cognition [72]. Taking inspiration from the {\textbackslash}saymind-independent versus {\textbackslash}saymind-dependent ontological distinction [4], we define sentences at Levels 1 and 2 as perceiver-independent (i.e., their content can be generated independently of human or machine perception, without reference to the visualization), while sentences at Level 3 are perceiver-dependent (i.e., their content requires a perceiver of some sort; likely a human, although machine perception may increasingly suffice for generating Level 3 content). Table 2 summarizes this distinction. 4.4 Level 4: Contextual and Domain-Specific Insights Finally, at the fourth level, there are sentences whose semantic content refers to contextual and domain-specific knowledge and experience. Consider the following two examples (Table 3.B.4 and 3.C.4). The big drop in prices was caused by financial crisis of 2007-2008. The crisis culminated with the bankruptcy of Lehman Brothers on September 15, 2008 and an international banking crisis. People living in low-income countries tend to have a lower life expectancy than the people living in high-income countries, likely due to many societal factors, including access to healthcare, food, other resources, and overall quality of life. These sentences convey social and political explanations for an observed trend that depends on an individual reader’s subjective knowledge about particular world events: the 2008 financial crisis and global socio-economic trends, respectively. This semantic content is characteristic of what is often referred to as “insight” in visualization research. Although lacking a precise and agreed-upon definition [60, 95, 76, 20, 61], an insight is often an observation about the data that is complex, deep, qualitative, unexpected, and relevant [108]. Critically, insights depend on individual perceivers, their subjective knowledge, and domain-expertise. Level 4 is where the breadth of an individual reader’s knowledge and experience is brought to bear in articulating something “insightful” about the visualized data. Computational Considerations. As with Levels 3, we say that Level 4 semantic content is perceiver-dependent, but in a stronger sense. This is because (setting aside consideration of hypothetical future “artificial general intelligence”) generating Level 4 semantic content is at-present a uniquely human endeavor. Doing so involves synthesizing background knowledge about the world (such as geographic, cultural, and political relationships between countries), contextual knowledge about current events (e.g., the fact that there was a global recession in 2008), and domain-specific knowledge (e.g., expertise in a particular field of research or scholarship). However, bespoke systems for narrowly-scoped domains (e.g., those auto-generating stock chart annotations using a corpus of human-authored news articles [45]) suggest that some Level 4 content might be feasibly generated sooner rather than later. Lastly, we briefly note that data-driven predictions can belong to either Level 2, 3, or 4, depending on the semantic content contained therein. For example: a point-wise prediction at Level 2 (e.g., computing a stock’s future expected price using the backing dataset); a prediction about future overall trends at Level 3 (e.g., observing that a steadily increasing stock price will likely continue to rise); a prediction involving contextual or domain-specific knowledge at Level 4 (e.g., the outcome of an election using a variety of poll data, social indicators, and political intuition). 5 Applying the Model: Evaluating the Effectiveness of Visualization Descriptions The foregoing conceptual model provides a means of making structured comparisons between different levels of semantic content and reader groups. To demonstrate how it can be applied to evaluate the effectiveness of visualization descriptions (i.e., whether or not they effectively convey meaningful information, and for whom), we conducted a mixed-methods evaluation in which 30 blind and 90 sighted readers first ranked the usefulness of descriptions authored at varying levels of semantic content, and then completed an open-ended questionnaire. 5.1 Evaluation Design We selected 15 visualizations for the evaluation, curated to be representative of the categories from our prior survey (§ 3). Specifically, we selected 5 visualizations for each of the three dimensions: type (bar, line, scatter), topic (academic, business, journalism), and difficulty (easy, medium, hard). For every visualization, participants were asked to rank the usefulness of 4 different descriptions, each corresponding to one level of semantic content, presented unlabeled and in random order. We piloted this rank-choice interface with 10 sighted readers recruited via Prolific and 1 blind reader, a non-academic collaborator proficient with Apple’s VoiceOver screen reader. Based on this pilot, we rewrote the study instructions to be more intelligible to both groups of readers, added an introductory example task to the evaluation, and improved the screen reader accessibility of our interface (e.g., by reordering nested dom elements to be more intuitively traversed by screen reader). In addition to curating a representative set of visualizations, we also curated descriptions representative of each level of semantic content. Participant-authored descriptions from our prior survey often did not contain content from all 4 levels or, if they did, this content was interleaved in a way that was not cleanly-separable for the purpose of a ranking task (Fig. 1). Thus, for this evaluation, we curated and collated sentences from multiple participant-authored descriptions to create exemplar descriptions, such that each text chunk contained only content belonging to a single semantic content level. Table 3.C shows one such exemplar description, whereas Table 3.A and B show the original un-collated descriptions. For each ranking task, readers were presented with a brief piece of contextualizing text, such as the following. {\textbackslash}saySuppose that you are reading an academic paper about how life expectancy differs for people of different genders from countries with different levels of income. You encounter the following visualization. [Table 3.C] Which content do you think would be most useful to include in a textual description of this visualization? Additionally, blind readers were presented with a brief text noting that the hypothetically-encountered visualization was inaccessible via screen reader technology. In contrast to prior work, which has evaluated chart descriptions in terms of “efficiency,” “informativeness,” and “clarity” [39, 78], we intentionally left the definition of “useful” open to the reader’s interpretation. We hypothesize that “useful” descriptions may not be necessarily efficient (i.e., they may require lengthy explanation or background context), and that both informativeness and clarity are constituents of usefulness. In short, ranking “usefulness” affords a holistic evaluation metric. Participants assigned usefulness rankings to each of the 4 descriptions by selecting corresponding radio buttons, labeled 1 (least useful) to 4 (most useful). In addition to these 4 descriptions, we included a 5th choice as an “attention check”: a sentence whose content was entirely irrelevant to the chart to ensure participants were reading each description prior to ranking them. If a participant did not rank the attention check as least useful, we filtered out their response from our final analysis. We include the evaluation interfaces and questions with the Supplemental Material. 5.2 Participants Participants consisted of two reader groups: 90 sighted readers recruited through the Prolific platform, and 30 blind readers recruited through our friends in the blind community and through a call for participation sent out via Twitter (n.b., in accessibility research, it is common to compare blind and sighted readers recruited through these means [14]). 5.2.1 Participant Recruitment For sighted readers qualifications for participation included English language proficiency and no color vision deficiency, and blind readers were expected to be proficient with a screen reader, such as Job Access With Speech (JAWS), NonVisual Desktop Access (NVDA), or Apple’s VoiceOver. Sighted readers were compensated at a rate of \$10-12 per hour, for an approximately 20-minute task. Blind readers were compensated at a rate of \$50 per hour, for an approximately 1-hour task. This difference in task duration was for two reasons. First, participants recruited through Prolific are usually not accustomed to completing lengthy tasks — our prior surveys and pilots suggested that these participants might contribute low-quality responses on “click-through” tasks if the task duration exceeded 15–20 minutes — and thus we asked each participant to rank only 5 of the 15 visualizations at a time. Second, given the difficulty of recruiting blind readers proficient with screen readers, we asked each blind participant to rank all 15 visualizations, and compensated them at a rate commensurate with their difficult-to-find expertise [67]. In this way, we recruited sufficient numbers of readers to ensure that each of the 15 visualization ranking tasks would be completed by 30 participants from both reader groups. 5.2.2 Participant Demographics Among the 30 blind participants, 53\% (n=16) reported their gender as male, 36\% (n=11) as female, and 3 participants “preferred not to say.” The most common highest level of education attained was a Bachelor’s degree (60\%, n=18), and most readers were between 20 – 40 years old (66\%, n=20). The screen reader technology readers used to complete the study was evenly balanced: VoiceOver (n=10), JAWS (n=10), NVDA (n=9), and “other” (n=1). Among the 90 sighted participants, 69\% reported their gender as male (n=62) and 31\% as female (n=28). The most common highest level of education attained was a high school diploma (42\%, n=38) followed by a Bachelor’s degree (40\%, n=36), and most sighted readers were between 20 – 30 years old (64\%, n=58). On a 7-point Likert scale [1=strongly disagree, 7=strongly agree], blind participants reported having “a good understanding of data visualization concepts” (μ=6.3, σ=1.03) as well as “a good understanding of statistical concepts and terminology” (μ=5.90, σ=1.01). Sighted participants reported similar levels of understanding: (μ=6.7, σ=0.73) and (μ=5.67, σ=1.06), respectively. Sighted participants also considered themselves to be “proficient at reading data visualizations” (μ=5.97, σ=0.89) and were able to “read and understand all of the visualizations presented in this study” (μ=6.44, σ=0.71). Table 4: (Upper) Rankings [1=least useful, 4=most useful] of semantic content at each level of the model, for blind and sighted readers. The scale encodes the number of times a given level was assigned a given rank by a reader. Dotted contour lines delineate Regions with a threshold equal to μ+σ2, each labeled with a capital letter A – F. (Lower) Shaded cells indicate significant ranking differences pair-wise between levels.  blind readers sighted readers levels 1×2 1×3 1×4 2×3 2×4 3×4 blind {\textbackslash}cellcoloryellow!25p{\textless}0.001 {\textbackslash}cellcoloryellow!25p{\textless}0.001 p{\textless}0.321 p{\textless}0.148 {\textbackslash}cellcoloryellow!25p{\textless}0.001 {\textbackslash}cellcoloryellow!25p{\textless}0.001 sighted {\textbackslash}cellcoloryellow!25p{\textless}0.001 {\textbackslash}cellcoloryellow!25p{\textless}0.001 {\textbackslash}cellcoloryellow!25p{\textless}0.001 {\textbackslash}cellcoloryellow!25p{\textless}0.001 {\textbackslash}cellcoloryellow!25p{\textless}0.001 p{\textless}0.059 5.3 Quantitative Results Quantitative results for the individual rankings (1,800 per blind and sighted reader groups) are summarized by the heatmaps in Table 4 (Upper), which aggregate the number of times a given content level was assigned a certain rank. Dotted lines in both blind and sighted heatmaps delineate regions exceeding a threshold — calculated by taking the mean plus half a standard deviation (μ+σ2) resulting in a value of 139 and 136, respectively — and are labeled with a capital letter A – F. These results exhibit significant differences between reader groups. For both reader groups, using Friedman’s Test (a non-parametric multi-comparison test for rank-order data) the p-value is p{\textless}0.001, so we reject the null hypothesis that the mean rank is the same for all four semantic content levels [37]. Additionally, in Table 4 (Lower), we find significant ranking differences when making pair-wise comparisons between levels, via Nemenyi’s test (a post-hoc test commonly coupled with Friedman’s to make pair-wise comparisons). There appears to be strong agreement among sighted readers that higher levels of semantic content are more useful: Levels 3 and 4 are found to be most useful (Region 4.F), while Levels 1 and 2 are least useful (Regions 4.D and  4.E). Blind readers agree with each other to a lesser extent, but strong trends are nevertheless apparent. In particular, blind readers rank content and Levels 2 and 3 as most useful (Region 4.C), and semantic content at Levels 1 and 4 as least useful (Regions 4.A and  4.B). When faceting these rankings by visualization type, topic, or difficulty we did not observe any significant differences, suggesting that both reader groups rank semantic content levels consistently, regardless of how the chart itself may vary. Noteworthy for both reader groups, the distribution of rankings for Level 1 is bimodal —– the only level to exhibit this property. While a vast majority of both blind and sighted readers rank Level 1 content as least useful, this level is ranked “most useful” in 101 and 87 instances by blind and sighted readers, respectively. This suggests that both reader groups have a more complicated perspective toward descriptions of a chart’s elemental and encoded properties; a finding we explore further by analyzing qualitative data. 5.4 Qualitative Results In a questionnaire, we asked readers to use a 7-point Likert scale [1=strongly disagree, 7=strongly agree] to rate their agreement with a set of statements about their experience with visualizations. We also asked them to offer open-ended feedback about which semantic content they found to be most useful and why. Here, we summarize the key trends that emerged from these two different forms of feedback, from both blind readers (BR) and sighted readers (SR). 5.4.1 Descriptions Are Important to Both Reader Groups All blind readers reported encountering inaccessible visualizations: either multiple times a week (43\%, n=13), everyday (20\%, n=6), once or twice a month (20\%, n=6), or at most once a week (17\%, n=5). These readers reported primarily encountering these barriers on social media (30\%, n=9), on newspaper websites (13\%, n=4), and in educational materials (53\%, n=16) — but, most often, barriers were encountered in all of the above contexts (53\%, n=16). Blind readers overwhelmingly agreed with the statements {\textbackslash}sayI often feel that important public information is inaccessible to me, because it is only available in a visual format (μ=6.1, σ=1.49), and {\textbackslash}sayProviding textual descriptions of data visualizations is important to me (μ=6.83, σ=0.38). “I am totally blind, and virtually all data visualizations I encounter are undescribed, and as such are unavailable. This has been acutely made clear on Twitter and in newspapers around the COVID-19 pandemic and the recent U.S. election. Often, visualizations are presented with very little introduction or coinciding text. I feel very left out of the world and left out of the ability to confidently traverse that world. The more data I am unable to access, the more vulnerable and devalued I feel.” (BR5) By contrast, sighted readers neither agreed nor disagreed regarding the inaccessibility of information conveyed visually (μ=4, σ=1.57). Similarly, they were split on whether they ever experienced barriers to reading visualizations, with 52\% (n=47) reporting that they sometimes do (especially when engaging with a new topic) and 48\% (n=43) reporting that they usually do not. Nevertheless, sighted readers expressed support for natural language descriptions of visualizations (μ=5.60, σ=1.27). A possible explanation for this support is that — regardless of whether the visualization is difficult to read — descriptions can still facilitate comprehension. For instance, SR64 noted that “textual description requires far less brainpower and can break down a seemingly complex visualization into an easy to grasp overview.” 5.4.2 Reader Groups Disagree About Contextual Content A majority of blind readers (63\%, n=19) were emphatic that descriptions should not contain an author’s subjective interpretations, contextual information, or editorializing about the visualized data (i.e., Level 4 content). Consistent with blind readers ranking this as among the least useful (Region 4.B), BR20 succinctly articulated a common sentiment: “I want the information to be simply laid out, not peppered with subjective commentary… I just prefer it to be straight facts, not presumptions or guesstimates.” BR4 also noted that an author’s “opinions” about the data “should absolutely be avoided,” and BR14 emphasized agency when interpreting data: “I want to have the time and space to interpret the numbers for myself before I read the analysis.” By contrast, many sighted readers 41\% (n=37) expressed the opposite sentiment (Region 4.F) noting that, for them, the most useful descriptions often “told a story,” communicated an important conclusion, or provided deeper insights into the visualized data. As SR64 noted: “A description that simply describes the visualization and its details is hardly useful, but a description that tells a story using the data and derives a solution from it is extremely useful.” Only 4\% (n=4) of sighted readers explicitly stated that a description should exclude Level 4 semantic content. 5.4.3 Some Readers Prefer Non-Statistical Content Overall, blind readers consistently ranked both Levels 2 and 3 as the most useful (Region 4.C). But, some readers explicitly expressed preference for the latter over the former, highlighting two distinguishing characteristics of Level 3 content: that it conveys not only descriptive statistics but overall perceptible trends, and that it is articulated in commonplace or “natural”-sounding language. For instance, BR26 remarked that a visualization description is “more useful if it contains the summary of the overall trends and distributions of the data rather than just mentioning some of the extreme values or means.” Similarly, BR21 noted that “not everyone who encounters a data visualization needs it for statistical purposes,” and further exclaimed “I want to know how a layperson sees it, not a statistician; I identify more with simpler terminology.” These preferences help to further delineate Level 3 from Levels 2 and 4. Content at Level 3 is “non-statistical” in the sense that it does only report statistical concepts and relations (as in Level 2), but neither does it do away with statistical “objectivity” entirely, so as to include subjective interpretation or speculation (as content in Level 4 might). In short, Level 3 content conveys statistically-grounded concepts in not-purely-statistical terms, a challenge that is core to visualization, and science communication more broadly. 5.4.4 Combinations of Content Levels Are Likely Most Useful While roughly 12\% readers from both blind and sighted groups indicated that a description should be as concise as possible, among blind readers, 40\% (n=12) noted that the most useful descriptions would combine content from multiple levels. This finding helps to explain the bimodality in Level 1 rankings we identified in the previous section. According to BR9, Level 1 content is only useful if other information is also conveyed: “All of the descriptions provided in this survey which *only* elaborated on x/y and color-coding are almost useless.” This sentiment was echoed by BR5, who added that if Level 1 content were “combined with the [Level 2 or Level 3], that’d make for a great description.” This finding has implications for research on automatic visualization captioning: these methods should aim to generate not only the lower levels of semantic content, but to more richly communicate a chart’s overall trends and statistics, sensitive to reader preferences. 5.4.5 Some Automatic Methods Raise Ethical Concerns Research on automatically generating visualization captions is often motivated by the goal of improving information access for people with visual disabilities [83, 27, 78, 84]. However, when deployed in real-world contexts, these methods may not confer their intended benefits, as one blind reader in our evaluation commented. “A.I. attempting to convert these images is still in its infancy. Facebook and Apple auto-descriptions of general images are more of a timewaster than useful. As a practical matter, if I find an inaccessible chart or graph, I just move on.” (BP22) Similarly, another participant (BR26) noted that if a description were to only describe a visualization’s encodings then “the reader wouldn’t get any insight from these texts, which not only increases the readers’ reading burden but also conveys no effective information about the data.” These sentiments reflect some of the ethical concerns surrounding the deployment of nascent CV and NLP models, which can output accurate but minimally informative content — or worse, can output erroneous content to a trusting audience [69, 78]. Facebook’s automatic image descriptions, for example, have been characterized by technology educator Chancey Fleet as {\textbackslash}sayfamously useless in the Blind community while {\textbackslash}saygarner[ing] a ton of glowing reviews from mainstream outlets without being of much use to disabled people [33, 40]. Such concerns might be mitigated by developing and evaluating automatic methods with disabled readers, through participatory design processes [67]. 6 Discussion and Future Work Our four-level model of semantic content — and its application to evaluating the usefulness of descriptions — has practical implications for the design of accessible data representations, and theoretical implications for the relationship between visualization and natural language. 6.1 Natural Language As An Interface Into Visualization Divergent reader preferences for semantic content suggests that it is helpful to think of natural language — not only as an interface for constructing and exploring visualizations [93, 36, 89] — but also as an interface into visualization, for understanding the semantic content they convey. Under this framing, we can apply Beaudoin-Lafon’s framework for evaluating interface models in terms of their descriptive, evaluative, and generative powers [7, 8], to bring further clarity to the practical design implications of our model. First, our grounded theory process yielded a model with descriptive power: it categorizes the semantic content conveyed by visualizations. Second, our study with blind and sighted readers demonstrated our model’s evaluative power: it offered a means of comparing different levels of semantic content, thus revealing divergent preferences between these different reader groups. Third, future work can now begin to study our model’s generative power: its implications for novel multimodal interfaces and accessible data representations. For instance, our evaluation suggested that descriptions primarily intending to benefit sighted readers might aim to generate higher-level semantic content (§ 5.4.2), while those intending to benefit blind readers might instead focus on affording readers the option to customize and combine different content levels (§ 5.4.4), depending on their individual preferences (§ 5.4.3). This latter path might involve automatically ARIA tagging web-based charts to surface semantic content at Levels 1 \& 2, with human-authors conveying Level 3 content. Or, it might involve applying our model to develop and evaluate the outputs of automatic captioning systems — to probe their technological capabilities and ethical implications — in collaboration with the relevant communities (§ 5.4.5). To facilitate this work, we have released our corpus of visualizations and labeled sentences under an open source license: vis.csail.mit.edu/pubs/vis-text-model/data/. 6.2 Natural Language As Coequal With Visualization In closing, we turn to a discussion of our model’s implications for visualization theory. Not only can we think of natural language as an interface into visualization (as above), but also as an interface into data itself; coequal with and complementary to visualization. For example, some semantic content (e.g., Level 2 statistics or Level 4 explanations) may be best conveyed via language, without any reference to visual modalities [82, 43], while other content (e.g., Level 3 clusters) may be uniquely suited to visual representation. This coequal framing is not a departure from orthodox visualization theory, but rather a return to its linguistic and semiotic origins. Indeed, at the start of his foundational Semiology of Graphics, Jacques Bertin introduces a similar framing to formalize an idea at the heart of visualization theory: content can be conveyed not only through speaking or writing but also through the “language” of graphics [12]. While Bertin took natural language as a point of departure for formalizing a language of graphics, we have here pursued the inverse: taking visualization as occasioning a return to language. This theoretical inversion opens avenues for future work, for which linguistic theory and semiotics are instructive [103, 68, 97]. Within the contemporary linguistic tradition, subfields like syntax, semantics, and pragmatics suggest opportunities for further analysis at each level of our model. And, since our model focuses on English sentences and canonical chart types, extensions to other languages and bespoke charts may be warranted. Within the semiotic tradition, Christian Metz (a contemporary of Bertin’s) emphasized the pluralistic quality of graphics [18]: the semantic content conveyed by visualizations depends not only on their graphical sign-system, but also on various “social codes” such as education, class, expertise, and — we hasten to include — ability. Our evaluation with blind and sighted readers (as well as work studying how charts are deployed in particular discourse contexts [44, 46, 62, 3]) lends credence to Metz’s conception of graphics as pluralistic: different readers will have different ideas about what makes visualizations meaningful (Fig. Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content). As a means of revealing these differences, we have here introduced a four-level model of semantic content. We leave further elucidation of the relationship between visualization and natural language to future work. Acknowledgements For their valuable feedback, we thank Emilie Gossiaux, Chancey Fleet, Michael Correll, Frank Elavsky, Beth Semel, Stephanie Tuerk, Crystal Lee, and the MIT Visualization Group. This work was supported by National Science Foundation GRFP-1122374 and III-1900991. References [1] P. Ackland, S. Resnikoff, and R. Bourne (2017) World Blindness and Visual Impairment. Community Eye Health. ISSN 0953-6833, Link Cited by: §1. [2] E. Adar and E. Lee (2020) Communicative Visualizations as a Learning Problem. In TVCG, (en). Cited by: §2.3.2. [3] G. Aiello (2020) Inventorizing, Situating, Transforming: Social Semiotics And Data Visualization. In Data Visualization in Society, M. Engebretsen and H. Kennedy (Eds.), Cited by: §6.2. [4] K. M. Ali (2016) Mind-Dependent Kinds. In Journal of Social Ontology, Cited by: §4.3. [5] R. Amar, J. Eagan, and J. Stasko (2005) Low-level Components Of Analytic Activity In Information Visualization. In INFOVIS, Cited by: §2.3.1. [6] A. Balaji, T. Ramanathan, and V. Sonathi (2018) Chart-Text: A Fully Automated Chart Image Descriptor. arXiv. Cited by: §2.1.1. [7] M. Beaudouin-Lafon (2000) Instrumental Interaction: An Interaction Model For Designing Post-WIMP User Interfaces. In CHI, , Link Cited by: §1, §6.1. [8] M. Beaudouin-Lafon (2004) Designing Interaction, Not Interfaces. In AVI, , Link Cited by: §1, §6.1. [9] Benetech Making Images Accessible. (en-US). Note: http://diagramcenter.org/making-images-accessible.html/ Link Cited by: §2.2.1. [10] C. L. Bennett, C. Gleason, M. K. Scheuerman, J. P. Bigham, A. Guo, and A. To (2021) “It’s Complicated”: Negotiating Accessibility and (Mis)Representation in Image Descriptions of Race, Gender, and Disability. In CHI, Cited by: §2.2.2. [11] C. T. Bergstrom (2020) SARS-CoV-2 Coronavirus. Note: http://ctbergstrom.com/covid19.html Link Cited by: Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content. [12] J. Bertin (1983) Semiology of Graphics. University of Wisconsin Press. Cited by: §1, §4.1, §6.2. [13] J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C. Miller, R. Miller, A. Tatarowicz, B. White, S. White, and T. Yeh (2010) VizWiz: Nearly Real-time Answers To Visual Questions. In UIST, Cited by: §2.2.2. [14] J. P. Bigham, I. Lin, and S. Savage (2017) The Effects of ”Not Knowing What You Don’t Know” on Web Accessibility for Blind Web Users. In ASSETS, Cited by: §5.2. [15] M. A. Borkin, Z. Bylinskii, N. W. Kim, C. M. Bainbridge, C. S. Yeh, D. Borkin, H. Pfister, and A. Oliva (2016) Beyond Memorability: Visualization Recognition and Recall. In TVCG, Cited by: §3.2.1. [16] M. A. Borkin, A. A. Vo, Z. Bylinskii, P. Isola, S. Sunkavalli, A. Oliva, and H. Pfister (2013) What Makes a Visualization Memorable?. In TVCG, Cited by: §3.2.1. [17] M. Brehmer and T. Munzner (2013) A Multi-Level Typology of Abstract Visualization Tasks. In TVCG, (en). Link Cited by: §2.3.1. [18] A. Campolo (2020) Signs and Sight: Jacques Bertin and the Visual Language of Structuralism. Grey Room. Link Cited by: §6.2. [19] A. Cesal (2020-08) Writing Alt Text for Data Visualization. (en). Link Cited by: §2.2.1. [20] R. Chang, C. Ziemkiewicz, T. M. Green, and W. Ribarsky (2009) Defining Insight for Visual Analytics. In CG\&A, Cited by: §4.4. [21] A. Chaparro and M. Chaparro (2017) Applications of Color in Design for Color-Deficient Users. Ergonomics in Design (en). ISSN 1064-8046, Link Cited by: §1. [22] C. Chen, R. Zhang, S. Kim, S. Cohen, T. Yu, R. Rossi, and R. Bunescu (2019) Neural Caption Generation Over Figures. In UbiComp/ISWC ’19 Adjunct, Link Cited by: §2.1.1. [23] C. Chen, R. Zhang, E. Koh, S. Kim, S. Cohen, and R. Rossi (2020) Figure Captioning with Relation Maps for Reasoning. In WACV, (en). Cited by: §2.1.1. [24] C. Chen, R. Zhang, E. Koh, S. Kim, S. Cohen, T. Yu, R. Rossi, and R. Bunescu (2019) Figure Captioning with Reasoning and Sequence-Level Training. arXiv. Link Cited by: §2.1.1. [25] J. Choi, S. Jung, D. G. Park, J. Choo, and N. Elmqvist (2019) Visualizing for the Non-Visual: Enabling the Visually Impaired to Use Visualization. In CGF, (en). Cited by: §1, §2.2. [26] S. Demir, S. Carberry, and K. F. McCoy (2008) Generating textual summaries of bar chartsGenerating Textual Summaries Of Bar Charts. In INLG, (en). Cited by: §4.2. [27] S. Demir, S. Carberry, and K. F. McCoy (2012) Summarizing Information Graphics Textually. In Computational Linguistics, Cited by: §1, §2.2, §2.3.1, §5.4.5. [28] M. Ehrenkranz (2020) Vital Coronavirus Information Is Failing the Blind and Visually Impaired. Vice. Link Cited by: §1, Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content. [29] F. Elavsky (2021) Chartability. Note: https://chartability.fizz.studio/ Cited by: §2.2.1. [30] S. Elzer, S. Carberry, D. Chester, S. Demir, N. Green, I. Zukerman, and K. Trnka (2005) Exploring And Exploiting The Limited Utility Of Captions In Recognizing Intention In Information Graphics. In ACL, Cited by: §2.2. [31] S. Elzer, E. Schwartz, S. Carberry, D. Chester, S. Demir, and P. Wu (2007) A Browser Extension For Providing Visually Impaired Users Access To The Content Of Bar Charts On The Web. In WEBIST, Cited by: §2.2. [32] C. Fisher (2019) Creating Accessible SVGs. (en-US). Link Cited by: §2.2.1. [33] C. Fleet (2021) Things which garner a ton of glowing reviews from mainstream outlets without being of much use to disabled people. For instance, Facebook’s auto image descriptions, much loved by sighted journos but famously useless in the Blind community. Twitter. Note: https://twitter.com/ChanceyFleet/status/1349211417744961536 Cited by: §5.4.5. [34] S. L. Fossheim (2020) An Introduction To Accessible Data Visualizations With D3.js. (en). Link Cited by: §2.2.1. [35] M. Galesic and R. Garcia-Retamero (2011) Graph Literacy: A Cross-cultural Comparison. In Medical Decision Making, Cited by: §2.3.1. [36] T. Gao, M. Dontcheva, E. Adar, Z. Liu, and K. G. Karahalios (2015) DataTone: Managing Ambiguity in Natural Language Interfaces for Data Visualization. In UIST, Cited by: §6.1. [37] S. García, A. Fernández, J. Luengo, and F. Herrera (2010) Advanced Nonparametric Tests For Multiple Comparisons In The Design Of Experiments In Computational Intelligence And Data Mining: Experimental Analysis Of Power. Information Sciences (en). Cited by: §5.3. [38] B. Geveci, W. Schroeder, A. Brown, and G. Wilson (2012) VTK. The Architecture of Open Source Applications. Cited by: §2.1.2. [39] B. Gould, T. O’Connell, and G. Freed (2008) Effective Practices for Description of Science Content within Digital Talking Books. Technical report The WGBH National Center for Accessible Media (en). Note: https://www.wgbh.org/foundation/ncam/guidelines/effective-practices-for-description-of-science-content-within-digital-talking-books Link Cited by: §1, §2.2.1, §5.1. [40] M. Hanley, S. Barocas, K. Levy, S. Azenkot, and H. Nissenbaum (2021) Computer Vision and Conflicting Values: Describing People with Automated Alt Text. arXiv. Cited by: §5.4.5. [41] L. Hasty, J. Milbury, I. Miller, A. O’Day, P. Acquinas, and D. Spence (2011) Guidelines and Standards for Tactile Graphics. Technical report Braille Authority of North America. Note: http://www.brailleauthority.org/tg/ Link Cited by: §1. [42] M. Hearst, M. Tory, and V. Setlur (2019) Toward Interface Defaults for Vague Modifiers in Natural Language Interfaces for Visual Analysis. In VIS, Cited by: §2.3.2. [43] M. Hearst and M. Tory (2019) Would You Like A Chart With That? Incorporating Visualizations into Conversational Interfaces. In VIS, Cited by: §2.3.2, §6.2. [44] J. Hullman and N. Diakopoulos (2011) Visualization Rhetoric: Framing Effects in Narrative Visualization. In TVCG, (en). Cited by: §6.2. [45] J. Hullman, N. Diakopoulos, and E. Adar (2013) Contextifier: automatic generation of annotated stock visualizations. In CHI, Cited by: §4.4. [46] J. Hullman, N. Diakopoulos, E. Momeni, and E. Adar (2015) Content, Context, and Critique: Commenting on a Data Visualization Blog. In CSCW, Cited by: §6.2. [47] S. E. Kahou, V. Michalski, A. Atkinson, A. Kadar, A. Trischler, and Y. Bengio (2018) FigureQA: An Annotated Figure Dataset for Visual Reasoning. arXiv. Cited by: §2.1.1, §4.1. [48] A. Karpathy and L. Fei-Fei (2017-04) Deep Visual-Semantic Alignments for Generating Image Descriptions. In TPAMI, Cited by: §2.1.1. [49] D. A. Keim and D. Oelke (2007) Literature Fingerprinting: A New Method for Visual Literary Analysis. In VAST, Cited by: Figure 1, §3.2.2. [50] D. H. Kim, E. Hoque, and M. Agrawala (2020-04) Answering Questions about Charts and Generating Visual Explanations. In CHI, (en). Cited by: §2.3.2. [51] D. H. Kim, E. Hoque, J. Kim, and M. Agrawala (2018) Facilitating Document Reading by Linking Text and Tables. In UIST, Cited by: §2.3.2. [52] D. H. Kim, V. Setlur, and M. Agrawala (2021) Towards Understanding How Readers Integrate Charts and Captions: A Case Study with Line Charts. In CHI, (en). Cited by: §2.3.2. [53] N. W. Kim, S. C. Joyner, A. Riegelhuth, and Y. Kim (2021) Accessible Visualization: Design Space, Opportunities, and Challenges. In CGF, Cited by: §1. [54] H. Kong, Z. Liu, and K. Karahalios (2018-04) Frames and Slants in Titles of Visualizations on Controversial Topics. In CHI, (en). Cited by: §2.3.2. [55] H. Kong, Z. Liu, and K. Karahalios (2019-05) Trust and Recall of Information across Varying Degrees of Title-Visualization Misalignment. In CHI, (en). Cited by: §2.3.2. [56] N. Kong, M. A. Hearst, and M. Agrawala (2014) Extracting References Between Text And Charts Via Crowdsourcing. In CHI, Cited by: §2.2.2. [57] S. M. Kosslyn (1989) Understanding Charts and Graphs. Applied Cognitive Psychology (en). Cited by: §2.3.1. [58] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei (2017) Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. In IJCV, Cited by: §2.1.1. [59] C. Lai, Z. Lin, R. Jiang, Y. Han, C. Liu, and X. Yuan (2020) Automatic Annotation Synchronizing with Textual Description for Visualization. In CHI, (en). Cited by: §2.1.1. [60] P. Law, A. Endert, and J. Stasko (2020-08) What are Data Insights to Professional Visualization Users?. arXiv. Link Cited by: §4.4. [61] P. Law, A. Endert, and J. Stasko (2020) Characterizing Automated Data Insights. arXiv. Cited by: §4.4. [62] C. Lee, T. Yang, G. Inchoco, G. M. Jones, and A. Satyanarayan (2021) Viral Visualizations: How Coronavirus Skeptics Use Orthodox Data Practices to Promote Unorthodox Science Online. In CHI, Cited by: §6.2. [63] S. Lee, S. Kim, and B. C. Kwon (2016) Vlat: Development Of A Visualization Literacy Assessment Test. In TVCG, Cited by: §2.3.1. [64] T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick (2014) Microsoft COCO: Common Objects in Context. In ECCV, (en). Cited by: §2.1.1. [65] T. Littlefield (2020) COVID-19 Statistics Tracker. (en). Note: https://cvstats.net Link Cited by: §1. [66] M. A. Livingston and D. Brock (2020) Position: Visual Sentences: Definitions and Applications. In VIS, Cited by: §2.3.1. [67] A. Lundgard, C. Lee, and A. Satyanarayan (2019-10) Sociotechnical Considerations for Accessible Visualization Design. In VIS, Cited by: §1, §2.2, §5.2.1, §5.4.5. [68] A. M. MacEachren, R. E. Roth, J. O’Brien, B. Li, D. Swingley, and M. Gahegan (2012) Visual Semiotics Uncertainty Visualization: An Empirical Study. In TVCG, Cited by: §6.2. [69] H. MacLeod, C. L. Bennett, M. R. Morris, and E. Cutrell (2017) Understanding Blind People’s Experiences with Computer-Generated Captions of Social Media Images. In CHI, Cited by: §2.1.1, §5.4.5. [70] J. Matejka and G. Fitzmaurice (2017) Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing. In CHI, (en). Cited by: §4.3. [71] P. Moraes, G. Sina, K. McCoy, and S. Carberry (2014) Evaluating The Accessibility Of Line Graphs Through Textual Summaries For Visually Impaired Users. In ASSETS, Cited by: §2.1.1, §2.2. [72] V. S. Morash, Y. Siu, J. A. Miele, L. Hasty, and S. Landau (2015) Guiding Novice Web Workers in Making Image Descriptions Using Templates. In TACCESS, Cited by: §2.1.1, §2.2.2, §4.3. [73] M. R. Morris, J. Johnson, C. L. Bennett, and E. Cutrell (2018) Rich Representations of Visual Content for Screen Reader Users. In CHI, Cited by: §2.2. [74] M. Muller (2014) Curiosity, Creativity, and Surprise as Analytic Tools: Grounded Theory Method. In Ways of Knowing in HCI, J. S. Olson and W. A. Kellogg (Eds.), Cited by: §3.1, §3. [75] A. Narechania, A. Srinivasan, and J. Stasko (2021) NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries. In TVCG, (en). Cited by: §2.3.2. [76] C. North (2006) Toward Measuring Visualization Insight. In CG\&A, Cited by: §4.4. [77] J. R. Nuñez, C. R. Anderton, and R. S. Renslow (2018) Optimizing Colormaps With Consideration For Color Vision Deficiency To Enable Accurate Interpretation Of Scientific Data. PLOS ONE (en). Cited by: §1. [78] J. Obeid and E. Hoque (2020) Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model. arXiv. Cited by: §1, §2.1.1, §2.2, §4.2, §4.3, §5.1, §5.4.5. [79] M. M. Oliveira (2013) Towards More Accessible Visualizations for Color-Vision-Deficient Individuals. In CiSE, Cited by: §1. [80] A. Ottley, A. Kaszowska, R. J. Crouser, and E. M. Peck (2019) The Curious Case of Combining Text and Visualization. In EuroVis, (en). Cited by: §2.3.2. [81] J. Poco and J. Heer (2017) Reverse-Engineering Visualizations: Recovering Visual Encodings from Chart Images. In CGF, Link Cited by: §3.2.1, §4.1. [82] V. Potluri, T. E. Grindeland, J. E. Froehlich, and J. Mankoff (2021) Examining Visual Semantic Understanding in Blind and Low-Vision Technology Users. In CHI, Cited by: §6.2. [83] X. Qian, E. Koh, F. Du, S. Kim, J. Chan, R. A. Rossi, S. Malik, and T. Y. Lee (2021) Generating Accurate Caption Units for Figure Captioning. In WWW, (en). Cited by: §2.1.1, §2.2, §5.4.5. [84] X. Qian, E. Koh, F. Du, S. Kim, and J. Chan (2020) A Formative Study on Designing Accurate and Natural Figure Captioning Systems. In CHI EA, Cited by: §1, §2.1.1, §2.2.2, §2.2, §2.3.1, §5.4.5. [85] J. M. Royer, C. N. Hastings, and C. Hook (1979) A Sentence Verification Technique For Measuring Reading Comprehension. Journal of Reading Behavior. Cited by: §2.3.1. [86] J. M. Royer (2001) Developing Reading And Listening Comprehension Tests Based On The Sentence Verification Technique (SVT). In Journal of Adolescent \& Adult Literacy, Cited by: §2.3.1. [87] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer (2017) Vega-Lite: A Grammar of Interactive Graphics. In TVCG, Cited by: §2.1.2, §3.2.1, §4.1. [88] D. Schepers (2020) Why Accessibility Is at the Heart of Data Visualization. (en). Link Cited by: §2.2.1. [89] V. Setlur, S. E. Battersby, M. Tory, R. Gossweiler, and A. X. Chang (2016) Eviza: A Natural Language Interface for Visual Analysis. In UIST, Cited by: §6.1. [90] V. Setlur, M. Tory, and A. Djalali (2019) Inferencing Underspecified Natural Language Utterances In Visual Analysis. In IUI, Cited by: §2.3.2. [91] A. Sharif, S. S. Chintalapati, J. O. Wobbrock, and K. Reinecke (2021) Understanding Screen-Reader Users’ Experiences with Online Data Visualizations. In ASSETS, Cited by: §2.2. [92] A. Srinivasan, S. M. Drucker, A. Endert, and J. Stasko (2019) Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication. In TVCG, (en). Cited by: §3.2.2, §4.2, §4.2. [93] A. Srinivasan, N. Nyapathy, B. Lee, S. M. Drucker, and J. Stasko (2021) Collecting and Characterizing Natural Language Utterances for Specifying Data Visualizations. In CHI, (en). Cited by: §2.3.2, §6.1. [94] H. Sutton (2020) Accessible Covid-19 Tracker Enables A Way For Visually Impaired To Stay Up To Date. Disability Compliance for Higher Education (en). Cited by: §1. [95] B. Tang, S. Han, M. L. Yiu, R. Ding, and D. Zhang (2017) Extracting Top-K Insights from Multi-dimensional Data. In SIGMOD, (en). Cited by: §4.4. [96] B. D. Team (2014) Bokeh: Python Library For Interactive Visualization. Bokeh Development Team. Cited by: §2.1.2. [97] P. Vickers, J. Faith, and N. Rossiter (2013) Understanding Visualization: A Formal Approach Using Category Theory and Semiotics. In TVCG, Cited by: §6.2. [98] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan (2015) Show and Tell: A Neural Image Caption Generator. In CVPR, (en). Link Cited by: §2.1.1. [99] W3C (2019) WAI Web Accessibility Tutorials: Complex Images. Note: https://www.w3.org/WAI/tutorials/images/complex/ Link Cited by: §1, §2.1.2, §2.2.1. [100] Y. Wang, Z. Sun, H. Zhang, W. Cui, K. Xu, X. Ma, and D. Zhang (2020) DataShot: Automatic Generation of Fact Sheets from Tabular Data. In TVCG, Cited by: §3.2.2, §4.2. [101] L. Watson (2017) Accessible SVG Line Graphs. (en). Note: https://tink.uk/accessible-svg-line-graphs/ Link Cited by: §2.2.1. [102] L. Watson (2018) Accessible SVG Flowcharts. (en). Link Cited by: §2.2.1. [103] W. Weber (2019) Towards a Semiotics of Data Visualization – an Inventory of Graphic Resources. In IV, Cited by: §6.2. [104] L. Wilkinson (2005) The Grammar of Graphics. Statistics and Computing, Springer-Verlag (en). Cited by: §4.1. [105] K. Wu, E. Petersen, T. Ahmad, D. Burlinson, S. Tanis, and D. A. Szafir (2021) Understanding Data Accessibility for People with Intellectual and Developmental Disabilities. In CHI 2021, (en). Cited by: §1. [106] C. Xiong, L. V. Weelden, and S. Franconeri (2020) The Curse of Knowledge in Visual Data Communication. In TVCG, Cited by: §2.3.2. [107] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio (2016) Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. arXiv. Cited by: §2.1.1. [108] J. S. Yi, Y. Kang, J. T. Stasko, and J. A. Jacko (2008) Understanding and Characterizing Insights: How Do People Gain Insights Using Information Visualization?. In BELIV, (en). Cited by: §4.4.},
	language = {en},
	urldate = {2021-10-29},
	author = {Lundgard, Alan and Satyanarayan, Arvind},
	month = jan,
	year = {2022},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/A54DNN9Y/Lundgard and Satyanarayan - 2022 - Accessible Visualization via Natural Language Desc.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/SKW2EHD7/vis-text-model.html:text/html},
}

@article{lundgard_accessible_2022,
	title = {Accessible {Visualization} via {Natural} {Language} {Descriptions}: {A} {Four}-{Level} {Model} of {Semantic} {Content}},
	shorttitle = {Accessible {Visualization} via {Natural} {Language} {Descriptions}},
	url = {http://vis.csail.mit.edu/pubs/vis-text-model/},
	doi = {10.1109/TVCG.2021.3114770},
	language = {en},
	urldate = {2021-10-29},
	author = {Lundgard, Alan and Satyanarayan, Arvind},
	month = jan,
	year = {2022}
}
